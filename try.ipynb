{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Annotated\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "DB_SEED_NUMBER = 42\n",
    "ELEMENT_SIZE = np.dtype(np.float32).itemsize\n",
    "DIMENSION = 70\n",
    "class CustomIndex: pass\n",
    "\n",
    "class VecDB:\n",
    "    def __init__(self, database_file_path = \"saved_db.dat\", index_file_path = \"index.dat\", new_db = True, db_size = None) -> None:\n",
    "        self.db_path = database_file_path\n",
    "        self.index_path = index_file_path\n",
    "        if new_db:\n",
    "            if db_size is None:\n",
    "                raise ValueError(\"You need to provide the size of the database\")\n",
    "            # delete the old DB file if exists\n",
    "            if os.path.exists(self.db_path):\n",
    "                os.remove(self.db_path)\n",
    "            self.generate_database(db_size)\n",
    "    \n",
    "    def generate_database(self, size: int) -> None:\n",
    "        rng = np.random.default_rng(DB_SEED_NUMBER)\n",
    "        vectors = rng.random((size, DIMENSION), dtype=np.float32)\n",
    "        self._write_vectors_to_file(vectors)\n",
    "        self._build_index()\n",
    "    # np.memmap() Parameters:\n",
    "    # First argument: File path to store data\n",
    "    # dtype: Data type (float32 here)\n",
    "    # mode: File access mode\n",
    "    # 'w+': Read/write, create if not exists\n",
    "    # shape: Dimensions of the array\n",
    "    # returns file contents on disk\n",
    "    def _write_vectors_to_file(self, vectors: np.ndarray) -> None:\n",
    "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='w+', shape=vectors.shape)\n",
    "        mmap_vectors[:] = vectors[:]\n",
    "        mmap_vectors.flush()\n",
    "\n",
    "    def _get_num_records(self) -> int:\n",
    "        return os.path.getsize(self.db_path) // (DIMENSION * ELEMENT_SIZE)\n",
    "\n",
    "    def insert_records(self, rows: Annotated[np.ndarray, (int, 70)]):\n",
    "        num_old_records = self._get_num_records()\n",
    "        num_new_records = len(rows)\n",
    "        full_shape = (num_old_records + num_new_records, DIMENSION)\n",
    "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='r+', shape=full_shape)\n",
    "        mmap_vectors[num_old_records:] = rows\n",
    "        mmap_vectors.flush()\n",
    "        #TODO: might change to call insert in the index, if you need\n",
    "        self._build_index()\n",
    "\n",
    "    def get_one_row(self, row_num: int) -> np.ndarray:\n",
    "        # This function is only load one row in memory\n",
    "        try:\n",
    "            offset = row_num * DIMENSION * ELEMENT_SIZE\n",
    "            # [0] is necessary because:\n",
    "            # mmap_vector is 2D array: [[x1, x2, ..., x70]]\n",
    "            # [0] extracts the single row as 1D: [x1, x2, ..., x70]\n",
    "            mmap_vector = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(1, DIMENSION), offset=offset)\n",
    "            return np.array(mmap_vector[0])\n",
    "        except Exception as e:\n",
    "            return f\"An error occurred: {e}\"\n",
    "\n",
    "    def get_all_rows(self) -> np.ndarray:\n",
    "        # Take care this load all the data in memory\n",
    "        num_records = self._get_num_records()\n",
    "        vectors = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(num_records, DIMENSION))\n",
    "        return np.array(vectors)\n",
    "    \n",
    "    def retrieve(self, query: Annotated[np.ndarray, (1, DIMENSION)], top_k = 5):\n",
    "        scores = []\n",
    "        num_records = self._get_num_records()\n",
    "        file = open(self.index_path,'rb')\n",
    "        index = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "        cluster_centers=index.centroids\n",
    "        labels_list=index.labels_list\n",
    "        for i,vec in enumerate(cluster_centers):\n",
    "            score=self._cal_score(query,vec)\n",
    "            scores.append((score,i))\n",
    "        # # here we assume that the row number is the ID of each vector\n",
    "        # for row_num in range(num_records):\n",
    "        #     vector = self.get_one_row(row_num)\n",
    "        #     score = self._cal_score(query, vector)\n",
    "        #     scores.append((score, row_num))\n",
    "        # here we assume that if two rows have the same score, return the lowest ID\n",
    "        scores = sorted(scores, reverse=True)[:top_k]\n",
    "        top_vector=[]\n",
    "        # for item in scores:\n",
    "        top_vector.append(labels_list[scores[0][1]])\n",
    "        return top_vector\n",
    "    \n",
    "    def _cal_score(self, vec1, vec2):\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm_vec1 = np.linalg.norm(vec1)\n",
    "        norm_vec2 = np.linalg.norm(vec2)\n",
    "        cosine_similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "        return cosine_similarity\n",
    "\n",
    "    def _build_index(self, num_clusters=100, num_subspaces=7):\n",
    "            \n",
    "            # Placeholder for index building logic\n",
    "            ### IVF\n",
    "            # Apply k means clustering to all vectors -> return the cluster centroids \n",
    "            # store the clusters and their centroids in a array or dictionary?\n",
    "            # Within each cluster:\n",
    "            # loop over each vector \n",
    "            # create an array of subspace arrays (parameter) subspaces[[   [subvector1 of array 1],[subvector1 of array 2]   ] , [],...]\n",
    "            # apply k-means clustering for each subspace[0] subspace[1] etc..\n",
    "            # Generate the codebook\n",
    "            \n",
    "            \n",
    "            # Build the PQ-IVF index.\n",
    "            # Args:\n",
    "            #     num_clusters (int): Number of clusters for the inverted file (IVF).\n",
    "            #     num_subspaces (int): Number of subspaces for product quantization.\n",
    "            #     subspace_dim (int): Dimension of each subspace.\n",
    "            # Sample dataset\n",
    "            vectors = self.get_all_rows()\n",
    "\n",
    "            # Step 1: Coarse Quantization (Clustering)\n",
    "            kmeans = KMeans(n_clusters=10)\n",
    "            labels = kmeans.fit_predict(vectors)  # Assign each vector to a cluster\n",
    "            cluster_centers = kmeans.cluster_centers_\n",
    "            # Step 2: Construct Posting Lists\n",
    "            labels_list = {i: [] for i in range(10)}  # Two clusters: 0 and 1\n",
    "            for i, label in enumerate(labels):\n",
    "                labels_list[label].append(i)\n",
    "            \n",
    "            CustomIndex.centroids=cluster_centers\n",
    "            CustomIndex.labels_list=labels_list\n",
    "            \n",
    "            filehandler = open(self.index_path,\"wb\")\n",
    "            pickle.dump(CustomIndex,filehandler)\n",
    "            filehandler.close()\n",
    "\n",
    "            file = open(self.index_path,'rb')\n",
    "            object_file = pickle.load(file)\n",
    "            file.close()\n",
    "\n",
    "            print(object_file.centroids, object_file.labels_list, sep=', ')\n",
    "            # Print Clustering Results\n",
    "            print(\"Cluster Centers:\", cluster_centers)\n",
    "            print(\"Labels:\", labels)\n",
    "            print(\"Posting Lists:\", labels_list)\n",
    "\n",
    "            # # # Query Processing\n",
    "            # query = np.random.randint(0, 10, (1, 7))  # Integers between 0 and 9\n",
    "            # nearest_centroid = np.argmin([np.linalg.norm(query - centroid) for centroid in kmeans.cluster_centers_])\n",
    "\n",
    "            # # Fine Search (Within the posting list of nearest centroid)\n",
    "            # nearest_vectors = [vectors[i] for i in labels_list[nearest_centroid]]\n",
    "            # closest_vector = min(nearest_vectors, key=lambda x: np.linalg.norm(query - x))\n",
    "            # print(f\"Actual vector: {query}\")\n",
    "            # print(f\"Nearest vector: {closest_vector}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.43187883 0.47953948 0.59312284 0.51649666 0.5528442  0.6288911\n",
      "  0.61080796 0.36431763 0.5185671  0.42126635 0.52380705 0.5567874\n",
      "  0.4839619  0.5286623  0.4352572  0.47157943 0.47131154 0.56470335\n",
      "  0.6302961  0.537825   0.37809974 0.5138853  0.70083207 0.6480595\n",
      "  0.5129867  0.3498488  0.39060143 0.43483418 0.6169812  0.5358724\n",
      "  0.5992363  0.4749908  0.374883   0.5177031  0.52992076 0.4877657\n",
      "  0.5008563  0.5430082  0.6113124  0.6271156  0.40733722 0.5960099\n",
      "  0.5131988  0.5418098  0.6339903  0.5182667  0.4143639  0.6351447\n",
      "  0.38917688 0.44540942 0.51121217 0.50888115 0.41288173 0.52871794\n",
      "  0.5001786  0.53358984 0.4973614  0.49575827 0.58931005 0.5123297\n",
      "  0.5515596  0.36735308 0.59534794 0.5151511  0.5949171  0.5432087\n",
      "  0.521006   0.39981484 0.5544974  0.5250895 ]\n",
      " [0.49499708 0.46210396 0.5207688  0.6439846  0.5247751  0.5852406\n",
      "  0.42157978 0.465078   0.5350113  0.4313518  0.46580195 0.49424586\n",
      "  0.40427485 0.48771155 0.42297724 0.46548316 0.4521501  0.45619315\n",
      "  0.45393512 0.4226768  0.52753997 0.5551609  0.64807796 0.5807549\n",
      "  0.39111412 0.4960517  0.59601074 0.46775335 0.6246674  0.43766475\n",
      "  0.35020596 0.5528983  0.52315044 0.43299943 0.37452462 0.6184439\n",
      "  0.31912112 0.5584003  0.42234492 0.4822338  0.51427805 0.5153285\n",
      "  0.64768225 0.5479537  0.5374459  0.44964913 0.3615679  0.4829501\n",
      "  0.5338664  0.43428254 0.38855445 0.47773156 0.6206077  0.67830676\n",
      "  0.5758952  0.5624554  0.59282506 0.50516814 0.6306536  0.40154102\n",
      "  0.51306033 0.60934794 0.46254882 0.437225   0.43923053 0.53991616\n",
      "  0.63259107 0.44054    0.43036717 0.51077926]\n",
      " [0.5099178  0.6635821  0.41130233 0.41826937 0.6098754  0.43030336\n",
      "  0.47760215 0.45516345 0.41641748 0.37721512 0.52975523 0.47092894\n",
      "  0.40878946 0.58311975 0.5905402  0.35397673 0.5605908  0.5339991\n",
      "  0.4884143  0.4827254  0.42987493 0.4161053  0.40746903 0.60467994\n",
      "  0.43574607 0.61269075 0.60707915 0.51958597 0.35106432 0.43799525\n",
      "  0.52312887 0.5729229  0.46684232 0.45061174 0.42750028 0.5031983\n",
      "  0.5479834  0.5523567  0.3348394  0.4424172  0.55593985 0.480293\n",
      "  0.5225317  0.53300774 0.61909354 0.47302705 0.5228614  0.47722033\n",
      "  0.54850155 0.49322218 0.59058726 0.36956814 0.5086762  0.3753261\n",
      "  0.55389583 0.4266625  0.46014574 0.5055283  0.48522007 0.47772044\n",
      "  0.50211257 0.5916755  0.61634666 0.52741104 0.36838052 0.6160759\n",
      "  0.55180806 0.4691369  0.50662667 0.5456567 ]\n",
      " [0.40806904 0.5539267  0.5514675  0.51523    0.64554846 0.31864452\n",
      "  0.57779634 0.46997803 0.4669928  0.54041666 0.38843644 0.46377143\n",
      "  0.5547026  0.525266   0.42303768 0.54914993 0.55935    0.4354447\n",
      "  0.44480538 0.29720885 0.60375434 0.5177477  0.49503323 0.5298183\n",
      "  0.5350291  0.56489944 0.5508888  0.5070237  0.46020296 0.61509913\n",
      "  0.5560977  0.53543544 0.6621357  0.48070198 0.46298778 0.4769528\n",
      "  0.460144   0.52698886 0.58911455 0.58929753 0.4300082  0.4693803\n",
      "  0.45631433 0.48145592 0.48686516 0.521574   0.55003756 0.45136952\n",
      "  0.39374554 0.5093523  0.5951627  0.47092742 0.41174507 0.53675526\n",
      "  0.54135936 0.41063726 0.4677811  0.5131337  0.54949373 0.47169083\n",
      "  0.52739674 0.44027707 0.5918279  0.51115847 0.5479372  0.3854782\n",
      "  0.51951015 0.53694457 0.4333626  0.49183917]\n",
      " [0.49478966 0.50875556 0.39695448 0.6024129  0.523569   0.548605\n",
      "  0.49500018 0.52535164 0.4765392  0.54187864 0.49133736 0.45787248\n",
      "  0.54758966 0.52473146 0.5101982  0.5859004  0.55999535 0.5027139\n",
      "  0.5226537  0.453359   0.544995   0.42311513 0.41039985 0.57296026\n",
      "  0.490399   0.41215077 0.48213917 0.48772064 0.4705049  0.46515092\n",
      "  0.42623    0.3744962  0.42311138 0.37827966 0.6019957  0.29934108\n",
      "  0.5240001  0.52276903 0.48642418 0.4643273  0.4999206  0.3635228\n",
      "  0.41882652 0.36383814 0.509182   0.44906527 0.49114093 0.59517395\n",
      "  0.58364624 0.5706317  0.42771682 0.61118174 0.3912109  0.52013284\n",
      "  0.3266793  0.43570465 0.56947255 0.4377832  0.3984214  0.50435936\n",
      "  0.7089341  0.5179495  0.39816996 0.4268636  0.50596535 0.69353557\n",
      "  0.480892   0.43078825 0.4399526  0.6152517 ]\n",
      " [0.5547412  0.5083804  0.5153209  0.4278376  0.40007076 0.5050691\n",
      "  0.48710144 0.5622915  0.5085107  0.4278543  0.51960367 0.5751055\n",
      "  0.6144024  0.41871598 0.48461196 0.54820347 0.4647717  0.5019139\n",
      "  0.5712193  0.46560326 0.46612537 0.60710514 0.3521316  0.4992324\n",
      "  0.5324661  0.5296356  0.51294583 0.49091974 0.41795564 0.4017973\n",
      "  0.49837446 0.60050756 0.3537651  0.5430585  0.5432912  0.4284707\n",
      "  0.5346869  0.46916506 0.49833938 0.5511646  0.38934267 0.5076536\n",
      "  0.5380937  0.57115    0.43477333 0.44108278 0.6244547  0.6294536\n",
      "  0.55601096 0.5293535  0.4006439  0.5305056  0.62878954 0.43316898\n",
      "  0.50966144 0.46964458 0.44060957 0.37197387 0.47576392 0.54936594\n",
      "  0.47087088 0.47250262 0.40435904 0.6084488  0.37590247 0.44172084\n",
      "  0.44421133 0.44974113 0.35393804 0.3609903 ]\n",
      " [0.37899214 0.39968103 0.56630915 0.49100396 0.38604674 0.53613514\n",
      "  0.57130903 0.51640576 0.542427   0.5372112  0.61994886 0.40617013\n",
      "  0.38294703 0.47452372 0.6097373  0.5283461  0.5387467  0.5885368\n",
      "  0.5609922  0.59774894 0.6046053  0.5791863  0.5419191  0.48506987\n",
      "  0.45977488 0.52555954 0.57769704 0.36306986 0.55541784 0.50973433\n",
      "  0.5010727  0.51169246 0.4770143  0.58146924 0.4941419  0.5642389\n",
      "  0.3854617  0.4490834  0.39726922 0.45568058 0.5782318  0.48976454\n",
      "  0.40361834 0.66186583 0.42849377 0.56935865 0.48460275 0.4974776\n",
      "  0.61179554 0.56796545 0.33428723 0.6034767  0.44265792 0.45802402\n",
      "  0.541406   0.4312907  0.4542598  0.5940139  0.54648364 0.44706893\n",
      "  0.3991804  0.40055892 0.50889355 0.49786475 0.56044585 0.40568453\n",
      "  0.39836365 0.477099   0.59696054 0.4044459 ]\n",
      " [0.4350014  0.4419274  0.5452116  0.60474485 0.4453541  0.5628956\n",
      "  0.56981826 0.4855146  0.5082555  0.5418032  0.45108303 0.52882904\n",
      "  0.37725824 0.48601803 0.52498126 0.43892807 0.45301333 0.39842656\n",
      "  0.50938565 0.67019504 0.54367787 0.50234056 0.4043815  0.393132\n",
      "  0.43702784 0.46383664 0.5144973  0.48944932 0.3458464  0.5612008\n",
      "  0.57486695 0.44937697 0.3886997  0.54804033 0.47443122 0.56882435\n",
      "  0.52086556 0.528784   0.4605937  0.56239486 0.59662294 0.5525431\n",
      "  0.43296087 0.306334   0.5014758  0.53437436 0.5433861  0.2859155\n",
      "  0.4847437  0.44100875 0.5055919  0.53119725 0.6333785  0.49033\n",
      "  0.43865132 0.48399973 0.5821854  0.50660956 0.4956519  0.50799274\n",
      "  0.608344   0.5669282  0.49009997 0.53539157 0.54363084 0.43394536\n",
      "  0.45543957 0.52591443 0.6508612  0.49366555]\n",
      " [0.46889308 0.47999394 0.4678642  0.39502135 0.49421328 0.51022494\n",
      "  0.36582145 0.5239886  0.55115277 0.5377821  0.52402914 0.53129065\n",
      "  0.38327974 0.5537918  0.5629684  0.5134379  0.5156831  0.4728138\n",
      "  0.39973933 0.6140207  0.36478475 0.48135126 0.52463675 0.4551274\n",
      "  0.47958022 0.54704803 0.45504066 0.5482213  0.5884601  0.46912232\n",
      "  0.48968402 0.40674484 0.690585   0.5114596  0.5796102  0.48339713\n",
      "  0.52307004 0.43847775 0.52531695 0.55861866 0.4536869  0.54028076\n",
      "  0.57310706 0.5643289  0.46642852 0.57784843 0.5699686  0.62924874\n",
      "  0.53867614 0.6232276  0.54024893 0.52691877 0.6100002  0.47934055\n",
      "  0.5265152  0.5386723  0.5183391  0.48667195 0.41198993 0.60304356\n",
      "  0.32050085 0.51712936 0.4338642  0.4461904  0.6464024  0.45686963\n",
      "  0.5553261  0.6619661  0.5390729  0.5114282 ]\n",
      " [0.6403321  0.4260863  0.42352554 0.43669197 0.40769172 0.5555015\n",
      "  0.392779   0.4976957  0.5050692  0.44584692 0.6130433  0.41135103\n",
      "  0.5743599  0.38160783 0.43203714 0.5289321  0.42977798 0.44989273\n",
      "  0.36808962 0.4639161  0.5065736  0.39423984 0.49040744 0.37540668\n",
      "  0.5270271  0.5013751  0.3730728  0.5731987  0.45520592 0.50775665\n",
      "  0.44453526 0.5039756  0.47779694 0.4423446  0.45736983 0.55377537\n",
      "  0.58834654 0.44805232 0.6601161  0.37633282 0.587395   0.55558234\n",
      "  0.5352643  0.5175675  0.5155516  0.39657775 0.50622374 0.44174486\n",
      "  0.45310652 0.49248207 0.46550614 0.4585667  0.385411   0.6480858\n",
      "  0.50777584 0.610153   0.48027357 0.49538845 0.43269575 0.61855865\n",
      "  0.47280246 0.4504091  0.4446782  0.54297274 0.39928153 0.5777427\n",
      "  0.5369672  0.5580796  0.5461248  0.52624977]], {0: [1, 19, 29, 51, 58, 59, 74, 81, 84, 104, 106, 135, 136, 143, 163, 165, 171, 189, 200, 205, 210, 214, 217, 235, 242, 249, 255, 266, 320, 321, 330, 339, 344, 350, 354, 356, 360, 369, 377, 386, 389, 395, 405, 411, 417, 419, 434, 435, 439, 470, 476, 486, 494, 510, 513, 516, 556, 562, 566, 585, 593, 603, 619, 657, 664, 667, 671, 672, 680, 685, 698, 719, 736, 737, 743, 757, 760, 778, 781, 795, 802, 805, 819, 840, 858, 866, 874, 880, 898, 906, 919, 920, 921, 950, 955, 976, 979, 986], 1: [5, 11, 14, 15, 17, 26, 66, 67, 71, 87, 112, 119, 128, 131, 152, 155, 168, 215, 232, 257, 261, 263, 278, 280, 301, 309, 362, 371, 396, 436, 445, 457, 467, 490, 491, 514, 523, 528, 537, 538, 543, 560, 561, 564, 565, 567, 584, 595, 601, 607, 620, 634, 652, 653, 703, 715, 717, 720, 729, 745, 758, 764, 790, 804, 810, 829, 832, 851, 875, 879, 891, 896, 901, 925, 935, 938, 944, 945, 953, 958, 960, 985, 993, 999], 2: [22, 28, 30, 35, 39, 40, 68, 76, 80, 93, 105, 140, 148, 161, 186, 199, 201, 208, 211, 220, 224, 226, 233, 236, 237, 238, 239, 243, 245, 254, 272, 274, 276, 279, 284, 289, 297, 307, 352, 373, 384, 387, 401, 431, 438, 452, 455, 464, 465, 468, 469, 471, 475, 492, 495, 496, 501, 504, 520, 531, 572, 574, 581, 597, 599, 609, 613, 622, 638, 644, 650, 659, 670, 673, 679, 686, 689, 728, 731, 733, 738, 750, 751, 763, 785, 791, 794, 796, 818, 869, 873, 878, 884, 889, 890, 907, 911, 914, 916, 941, 949, 969, 971, 978, 988, 998], 3: [3, 6, 7, 21, 42, 53, 54, 72, 73, 83, 85, 94, 100, 102, 110, 114, 116, 123, 129, 130, 156, 167, 175, 177, 181, 197, 231, 234, 262, 268, 291, 304, 308, 316, 331, 336, 341, 381, 390, 397, 406, 424, 430, 437, 458, 477, 487, 488, 521, 522, 527, 530, 535, 551, 553, 554, 563, 610, 625, 627, 636, 639, 641, 642, 658, 675, 684, 688, 705, 706, 708, 710, 712, 721, 724, 727, 756, 771, 782, 787, 788, 792, 799, 800, 814, 820, 827, 833, 841, 857, 882, 886, 894, 908, 918, 939, 943, 952, 954, 959, 968, 981, 982, 990, 992, 996], 4: [23, 52, 86, 113, 115, 118, 121, 122, 133, 154, 169, 170, 183, 188, 193, 206, 228, 258, 259, 281, 285, 294, 310, 313, 317, 318, 323, 329, 347, 348, 353, 361, 363, 366, 380, 392, 402, 403, 409, 461, 484, 524, 525, 548, 559, 608, 612, 617, 635, 645, 655, 656, 665, 697, 707, 714, 716, 718, 726, 739, 754, 801, 807, 817, 824, 825, 836, 862, 867, 868, 871, 872, 902, 903, 933, 937, 942, 967, 974, 991], 5: [0, 8, 12, 13, 20, 25, 27, 36, 44, 46, 57, 60, 61, 88, 89, 90, 96, 101, 103, 109, 138, 141, 159, 172, 173, 182, 207, 216, 222, 223, 241, 246, 256, 260, 267, 302, 325, 334, 335, 338, 345, 364, 372, 376, 391, 418, 423, 446, 460, 466, 478, 479, 480, 489, 497, 499, 502, 507, 517, 533, 534, 549, 550, 571, 577, 582, 588, 615, 618, 624, 637, 640, 648, 660, 693, 722, 740, 752, 761, 767, 769, 773, 774, 775, 784, 798, 812, 837, 842, 843, 850, 864, 865, 895, 924, 926, 928, 936, 946, 956, 962, 977, 980, 987, 989, 994], 6: [16, 18, 32, 34, 37, 38, 48, 50, 56, 62, 65, 70, 75, 111, 134, 137, 142, 151, 185, 195, 202, 204, 209, 212, 213, 240, 293, 305, 319, 326, 328, 337, 355, 368, 379, 382, 383, 398, 422, 443, 444, 447, 474, 485, 508, 511, 545, 546, 547, 568, 569, 580, 583, 586, 587, 592, 598, 600, 616, 632, 647, 668, 683, 691, 694, 742, 746, 753, 762, 765, 783, 793, 821, 822, 826, 830, 834, 852, 893, 915, 927, 931, 947, 948, 965, 972], 7: [9, 43, 45, 63, 77, 91, 99, 108, 120, 125, 132, 144, 147, 149, 153, 157, 158, 178, 180, 184, 194, 203, 227, 230, 244, 248, 264, 265, 270, 273, 287, 290, 292, 295, 306, 332, 333, 342, 359, 367, 385, 388, 393, 399, 407, 410, 415, 416, 421, 425, 428, 442, 451, 456, 459, 463, 473, 481, 483, 498, 500, 505, 506, 512, 532, 558, 573, 590, 596, 604, 626, 649, 661, 662, 669, 676, 678, 682, 687, 690, 692, 704, 709, 711, 723, 748, 768, 772, 786, 809, 816, 823, 828, 831, 844, 853, 854, 861, 863, 877, 881, 905, 909, 912, 922, 923, 966, 975, 983, 995, 997], 8: [33, 41, 47, 49, 55, 69, 79, 95, 97, 117, 124, 127, 139, 145, 146, 150, 162, 164, 166, 176, 179, 187, 190, 196, 198, 251, 252, 275, 282, 283, 288, 298, 300, 311, 312, 314, 324, 327, 340, 343, 349, 351, 357, 358, 370, 394, 400, 404, 413, 414, 420, 427, 429, 448, 453, 454, 462, 503, 515, 519, 526, 536, 552, 555, 557, 576, 579, 591, 605, 614, 621, 630, 631, 646, 651, 663, 674, 681, 695, 699, 700, 702, 734, 735, 744, 747, 755, 770, 777, 779, 780, 806, 811, 815, 846, 847, 860, 870, 876, 883, 885, 892, 899, 900, 913, 917, 930, 932, 940, 951, 957, 961, 963, 964, 984], 9: [2, 4, 10, 24, 31, 64, 78, 82, 92, 98, 107, 126, 160, 174, 191, 192, 218, 219, 221, 225, 229, 247, 250, 253, 269, 271, 277, 286, 296, 299, 303, 315, 322, 346, 365, 374, 375, 378, 408, 412, 426, 432, 433, 440, 441, 449, 450, 472, 482, 493, 509, 518, 529, 539, 540, 541, 542, 544, 570, 575, 578, 589, 594, 602, 606, 611, 623, 628, 629, 633, 643, 654, 666, 677, 696, 701, 713, 725, 730, 732, 741, 749, 759, 766, 776, 789, 797, 803, 808, 813, 835, 838, 839, 845, 848, 849, 855, 856, 859, 887, 888, 897, 904, 910, 929, 934, 970, 973]}\n",
      "Cluster Centers: [[0.43187883 0.47953948 0.59312284 0.51649666 0.5528442  0.6288911\n",
      "  0.61080796 0.36431763 0.5185671  0.42126635 0.52380705 0.5567874\n",
      "  0.4839619  0.5286623  0.4352572  0.47157943 0.47131154 0.56470335\n",
      "  0.6302961  0.537825   0.37809974 0.5138853  0.70083207 0.6480595\n",
      "  0.5129867  0.3498488  0.39060143 0.43483418 0.6169812  0.5358724\n",
      "  0.5992363  0.4749908  0.374883   0.5177031  0.52992076 0.4877657\n",
      "  0.5008563  0.5430082  0.6113124  0.6271156  0.40733722 0.5960099\n",
      "  0.5131988  0.5418098  0.6339903  0.5182667  0.4143639  0.6351447\n",
      "  0.38917688 0.44540942 0.51121217 0.50888115 0.41288173 0.52871794\n",
      "  0.5001786  0.53358984 0.4973614  0.49575827 0.58931005 0.5123297\n",
      "  0.5515596  0.36735308 0.59534794 0.5151511  0.5949171  0.5432087\n",
      "  0.521006   0.39981484 0.5544974  0.5250895 ]\n",
      " [0.49499708 0.46210396 0.5207688  0.6439846  0.5247751  0.5852406\n",
      "  0.42157978 0.465078   0.5350113  0.4313518  0.46580195 0.49424586\n",
      "  0.40427485 0.48771155 0.42297724 0.46548316 0.4521501  0.45619315\n",
      "  0.45393512 0.4226768  0.52753997 0.5551609  0.64807796 0.5807549\n",
      "  0.39111412 0.4960517  0.59601074 0.46775335 0.6246674  0.43766475\n",
      "  0.35020596 0.5528983  0.52315044 0.43299943 0.37452462 0.6184439\n",
      "  0.31912112 0.5584003  0.42234492 0.4822338  0.51427805 0.5153285\n",
      "  0.64768225 0.5479537  0.5374459  0.44964913 0.3615679  0.4829501\n",
      "  0.5338664  0.43428254 0.38855445 0.47773156 0.6206077  0.67830676\n",
      "  0.5758952  0.5624554  0.59282506 0.50516814 0.6306536  0.40154102\n",
      "  0.51306033 0.60934794 0.46254882 0.437225   0.43923053 0.53991616\n",
      "  0.63259107 0.44054    0.43036717 0.51077926]\n",
      " [0.5099178  0.6635821  0.41130233 0.41826937 0.6098754  0.43030336\n",
      "  0.47760215 0.45516345 0.41641748 0.37721512 0.52975523 0.47092894\n",
      "  0.40878946 0.58311975 0.5905402  0.35397673 0.5605908  0.5339991\n",
      "  0.4884143  0.4827254  0.42987493 0.4161053  0.40746903 0.60467994\n",
      "  0.43574607 0.61269075 0.60707915 0.51958597 0.35106432 0.43799525\n",
      "  0.52312887 0.5729229  0.46684232 0.45061174 0.42750028 0.5031983\n",
      "  0.5479834  0.5523567  0.3348394  0.4424172  0.55593985 0.480293\n",
      "  0.5225317  0.53300774 0.61909354 0.47302705 0.5228614  0.47722033\n",
      "  0.54850155 0.49322218 0.59058726 0.36956814 0.5086762  0.3753261\n",
      "  0.55389583 0.4266625  0.46014574 0.5055283  0.48522007 0.47772044\n",
      "  0.50211257 0.5916755  0.61634666 0.52741104 0.36838052 0.6160759\n",
      "  0.55180806 0.4691369  0.50662667 0.5456567 ]\n",
      " [0.40806904 0.5539267  0.5514675  0.51523    0.64554846 0.31864452\n",
      "  0.57779634 0.46997803 0.4669928  0.54041666 0.38843644 0.46377143\n",
      "  0.5547026  0.525266   0.42303768 0.54914993 0.55935    0.4354447\n",
      "  0.44480538 0.29720885 0.60375434 0.5177477  0.49503323 0.5298183\n",
      "  0.5350291  0.56489944 0.5508888  0.5070237  0.46020296 0.61509913\n",
      "  0.5560977  0.53543544 0.6621357  0.48070198 0.46298778 0.4769528\n",
      "  0.460144   0.52698886 0.58911455 0.58929753 0.4300082  0.4693803\n",
      "  0.45631433 0.48145592 0.48686516 0.521574   0.55003756 0.45136952\n",
      "  0.39374554 0.5093523  0.5951627  0.47092742 0.41174507 0.53675526\n",
      "  0.54135936 0.41063726 0.4677811  0.5131337  0.54949373 0.47169083\n",
      "  0.52739674 0.44027707 0.5918279  0.51115847 0.5479372  0.3854782\n",
      "  0.51951015 0.53694457 0.4333626  0.49183917]\n",
      " [0.49478966 0.50875556 0.39695448 0.6024129  0.523569   0.548605\n",
      "  0.49500018 0.52535164 0.4765392  0.54187864 0.49133736 0.45787248\n",
      "  0.54758966 0.52473146 0.5101982  0.5859004  0.55999535 0.5027139\n",
      "  0.5226537  0.453359   0.544995   0.42311513 0.41039985 0.57296026\n",
      "  0.490399   0.41215077 0.48213917 0.48772064 0.4705049  0.46515092\n",
      "  0.42623    0.3744962  0.42311138 0.37827966 0.6019957  0.29934108\n",
      "  0.5240001  0.52276903 0.48642418 0.4643273  0.4999206  0.3635228\n",
      "  0.41882652 0.36383814 0.509182   0.44906527 0.49114093 0.59517395\n",
      "  0.58364624 0.5706317  0.42771682 0.61118174 0.3912109  0.52013284\n",
      "  0.3266793  0.43570465 0.56947255 0.4377832  0.3984214  0.50435936\n",
      "  0.7089341  0.5179495  0.39816996 0.4268636  0.50596535 0.69353557\n",
      "  0.480892   0.43078825 0.4399526  0.6152517 ]\n",
      " [0.5547412  0.5083804  0.5153209  0.4278376  0.40007076 0.5050691\n",
      "  0.48710144 0.5622915  0.5085107  0.4278543  0.51960367 0.5751055\n",
      "  0.6144024  0.41871598 0.48461196 0.54820347 0.4647717  0.5019139\n",
      "  0.5712193  0.46560326 0.46612537 0.60710514 0.3521316  0.4992324\n",
      "  0.5324661  0.5296356  0.51294583 0.49091974 0.41795564 0.4017973\n",
      "  0.49837446 0.60050756 0.3537651  0.5430585  0.5432912  0.4284707\n",
      "  0.5346869  0.46916506 0.49833938 0.5511646  0.38934267 0.5076536\n",
      "  0.5380937  0.57115    0.43477333 0.44108278 0.6244547  0.6294536\n",
      "  0.55601096 0.5293535  0.4006439  0.5305056  0.62878954 0.43316898\n",
      "  0.50966144 0.46964458 0.44060957 0.37197387 0.47576392 0.54936594\n",
      "  0.47087088 0.47250262 0.40435904 0.6084488  0.37590247 0.44172084\n",
      "  0.44421133 0.44974113 0.35393804 0.3609903 ]\n",
      " [0.37899214 0.39968103 0.56630915 0.49100396 0.38604674 0.53613514\n",
      "  0.57130903 0.51640576 0.542427   0.5372112  0.61994886 0.40617013\n",
      "  0.38294703 0.47452372 0.6097373  0.5283461  0.5387467  0.5885368\n",
      "  0.5609922  0.59774894 0.6046053  0.5791863  0.5419191  0.48506987\n",
      "  0.45977488 0.52555954 0.57769704 0.36306986 0.55541784 0.50973433\n",
      "  0.5010727  0.51169246 0.4770143  0.58146924 0.4941419  0.5642389\n",
      "  0.3854617  0.4490834  0.39726922 0.45568058 0.5782318  0.48976454\n",
      "  0.40361834 0.66186583 0.42849377 0.56935865 0.48460275 0.4974776\n",
      "  0.61179554 0.56796545 0.33428723 0.6034767  0.44265792 0.45802402\n",
      "  0.541406   0.4312907  0.4542598  0.5940139  0.54648364 0.44706893\n",
      "  0.3991804  0.40055892 0.50889355 0.49786475 0.56044585 0.40568453\n",
      "  0.39836365 0.477099   0.59696054 0.4044459 ]\n",
      " [0.4350014  0.4419274  0.5452116  0.60474485 0.4453541  0.5628956\n",
      "  0.56981826 0.4855146  0.5082555  0.5418032  0.45108303 0.52882904\n",
      "  0.37725824 0.48601803 0.52498126 0.43892807 0.45301333 0.39842656\n",
      "  0.50938565 0.67019504 0.54367787 0.50234056 0.4043815  0.393132\n",
      "  0.43702784 0.46383664 0.5144973  0.48944932 0.3458464  0.5612008\n",
      "  0.57486695 0.44937697 0.3886997  0.54804033 0.47443122 0.56882435\n",
      "  0.52086556 0.528784   0.4605937  0.56239486 0.59662294 0.5525431\n",
      "  0.43296087 0.306334   0.5014758  0.53437436 0.5433861  0.2859155\n",
      "  0.4847437  0.44100875 0.5055919  0.53119725 0.6333785  0.49033\n",
      "  0.43865132 0.48399973 0.5821854  0.50660956 0.4956519  0.50799274\n",
      "  0.608344   0.5669282  0.49009997 0.53539157 0.54363084 0.43394536\n",
      "  0.45543957 0.52591443 0.6508612  0.49366555]\n",
      " [0.46889308 0.47999394 0.4678642  0.39502135 0.49421328 0.51022494\n",
      "  0.36582145 0.5239886  0.55115277 0.5377821  0.52402914 0.53129065\n",
      "  0.38327974 0.5537918  0.5629684  0.5134379  0.5156831  0.4728138\n",
      "  0.39973933 0.6140207  0.36478475 0.48135126 0.52463675 0.4551274\n",
      "  0.47958022 0.54704803 0.45504066 0.5482213  0.5884601  0.46912232\n",
      "  0.48968402 0.40674484 0.690585   0.5114596  0.5796102  0.48339713\n",
      "  0.52307004 0.43847775 0.52531695 0.55861866 0.4536869  0.54028076\n",
      "  0.57310706 0.5643289  0.46642852 0.57784843 0.5699686  0.62924874\n",
      "  0.53867614 0.6232276  0.54024893 0.52691877 0.6100002  0.47934055\n",
      "  0.5265152  0.5386723  0.5183391  0.48667195 0.41198993 0.60304356\n",
      "  0.32050085 0.51712936 0.4338642  0.4461904  0.6464024  0.45686963\n",
      "  0.5553261  0.6619661  0.5390729  0.5114282 ]\n",
      " [0.6403321  0.4260863  0.42352554 0.43669197 0.40769172 0.5555015\n",
      "  0.392779   0.4976957  0.5050692  0.44584692 0.6130433  0.41135103\n",
      "  0.5743599  0.38160783 0.43203714 0.5289321  0.42977798 0.44989273\n",
      "  0.36808962 0.4639161  0.5065736  0.39423984 0.49040744 0.37540668\n",
      "  0.5270271  0.5013751  0.3730728  0.5731987  0.45520592 0.50775665\n",
      "  0.44453526 0.5039756  0.47779694 0.4423446  0.45736983 0.55377537\n",
      "  0.58834654 0.44805232 0.6601161  0.37633282 0.587395   0.55558234\n",
      "  0.5352643  0.5175675  0.5155516  0.39657775 0.50622374 0.44174486\n",
      "  0.45310652 0.49248207 0.46550614 0.4585667  0.385411   0.6480858\n",
      "  0.50777584 0.610153   0.48027357 0.49538845 0.43269575 0.61855865\n",
      "  0.47280246 0.4504091  0.4446782  0.54297274 0.39928153 0.5777427\n",
      "  0.5369672  0.5580796  0.5461248  0.52624977]]\n",
      "Labels: [5 0 9 3 9 1 3 3 5 7 9 1 5 5 1 1 6 1 6 0 5 3 2 4 9 5 1 5 2 0 2 9 6 8 6 2 5\n",
      " 6 6 2 2 8 3 7 5 7 5 8 6 8 6 0 4 3 3 8 6 5 0 0 5 5 6 7 9 6 1 1 2 8 6 1 3 3\n",
      " 0 6 2 7 9 8 2 0 9 3 0 3 4 1 5 5 5 7 9 2 3 8 5 8 9 7 3 5 3 5 0 2 0 9 7 5 3\n",
      " 6 1 4 3 4 3 8 4 1 7 4 4 3 8 7 9 8 1 3 3 1 7 4 6 0 0 6 5 8 2 5 6 0 7 8 8 7\n",
      " 2 7 8 6 1 7 4 1 3 7 7 5 9 2 8 0 8 0 8 3 1 4 4 0 5 5 9 3 8 3 7 8 7 3 5 4 7\n",
      " 6 2 8 4 0 8 9 9 4 7 6 8 3 8 2 0 2 6 7 6 0 4 5 2 6 0 2 6 6 0 1 5 0 9 9 2 9\n",
      " 5 5 2 9 2 7 4 9 7 3 1 2 3 0 2 2 2 2 6 5 0 2 7 2 5 9 7 0 9 8 8 9 2 0 5 1 4\n",
      " 4 5 1 3 1 7 7 0 5 3 9 7 9 2 7 2 8 2 9 1 2 1 4 8 8 2 4 9 7 8 2 7 3 7 6 4 7\n",
      " 9 2 8 9 8 1 5 9 3 6 7 2 3 1 4 8 8 4 8 9 3 4 4 6 0 0 9 4 8 5 6 8 6 4 0 3 7\n",
      " 7 5 5 3 6 5 0 8 3 7 8 0 5 9 4 4 8 0 8 2 4 0 6 0 8 8 7 0 4 1 4 5 9 4 7 6 0\n",
      " 8 1 5 2 9 9 5 0 9 6 4 3 6 6 2 7 0 2 7 0 3 5 4 7 8 0 1 3 6 7 8 2 4 4 8 0 3\n",
      " 7 9 4 7 0 9 8 8 7 7 0 5 0 8 7 6 5 3 7 9 8 7 8 3 2 9 9 0 0 1 3 2 0 9 9 7 6\n",
      " 6 1 5 6 8 9 9 7 2 8 8 2 7 1 3 7 5 4 8 7 2 2 5 1 2 2 0 2 9 7 6 2 0 3 5 5 5\n",
      " 7 9 7 4 6 0 3 3 5 1 1 2 9 0 2 2 5 7 5 7 2 5 8 2 7 7 5 6 9 0 6 7 0 1 8 0 5\n",
      " 9 8 2 3 3 1 4 4 8 3 1 9 3 2 7 5 5 3 8 1 1 9 9 9 9 1 9 6 6 6 4 5 5 3 8 3 3\n",
      " 8 0 8 7 4 1 1 0 3 1 1 0 1 6 6 9 5 2 7 2 9 8 5 9 8 6 2 5 6 1 0 6 6 5 9 7 8\n",
      " 6 0 9 1 7 2 6 2 6 1 9 0 7 8 9 1 4 2 3 9 4 2 8 5 6 4 5 0 1 8 2 9 5 3 7 3 9\n",
      " 9 8 8 6 9 1 4 3 5 2 3 5 3 3 9 2 4 8 6 5 7 2 8 1 1 9 4 4 0 3 2 5 7 7 8 0 4\n",
      " 9 0 6 7 2 0 0 2 8 3 7 9 7 2 0 8 7 6 3 0 2 7 3 2 7 6 7 5 6 8 9 4 0 8 8 9 8\n",
      " 1 7 3 3 4 3 7 3 7 3 9 4 1 4 1 4 0 1 3 5 7 3 9 4 3 2 1 9 2 9 2 8 8 0 0 2 4\n",
      " 5 9 6 0 8 1 6 8 7 9 2 2 5 6 4 8 3 0 1 9 0 5 6 2 1 6 9 5 7 5 8 3 7 5 5 5 9\n",
      " 8 0 8 8 0 3 6 5 2 7 3 3 9 1 2 3 6 2 0 2 9 5 3 3 4 0 9 1 0 8 4 9 7 1 8 5 9\n",
      " 3 8 7 4 2 0 3 6 6 7 4 4 6 3 7 1 6 7 1 3 6 9 4 5 9 9 0 3 5 5 7 9 8 8 9 9 5\n",
      " 1 6 7 7 9 9 3 0 9 8 7 4 7 5 5 0 4 4 2 8 4 4 2 0 1 8 7 2 1 0 7 3 8 2 8 3 9\n",
      " 9 2 2 1 8 6 3 5 1 9 0 8 8 1 4 4 9 7 0 2 3 7 9 2 7 8 2 6 2 8 3 0 0 0 7 7 5\n",
      " 1 5 6 5 9 8 6 8 4 9 1 5 4 1 3 8 2 4 3 1 1 5 6 6 2 0 8 3 1 3 0 5 8 1 3 1 8\n",
      " 5 8 8 6 7 4 3 2 9 2 6 9 4 7 0 5 2 0 5 3 3 7 8 1 0 5 2 5 3 4 3 1 5 7 3 7 2\n",
      " 1]\n",
      "Posting Lists: {0: [1, 19, 29, 51, 58, 59, 74, 81, 84, 104, 106, 135, 136, 143, 163, 165, 171, 189, 200, 205, 210, 214, 217, 235, 242, 249, 255, 266, 320, 321, 330, 339, 344, 350, 354, 356, 360, 369, 377, 386, 389, 395, 405, 411, 417, 419, 434, 435, 439, 470, 476, 486, 494, 510, 513, 516, 556, 562, 566, 585, 593, 603, 619, 657, 664, 667, 671, 672, 680, 685, 698, 719, 736, 737, 743, 757, 760, 778, 781, 795, 802, 805, 819, 840, 858, 866, 874, 880, 898, 906, 919, 920, 921, 950, 955, 976, 979, 986], 1: [5, 11, 14, 15, 17, 26, 66, 67, 71, 87, 112, 119, 128, 131, 152, 155, 168, 215, 232, 257, 261, 263, 278, 280, 301, 309, 362, 371, 396, 436, 445, 457, 467, 490, 491, 514, 523, 528, 537, 538, 543, 560, 561, 564, 565, 567, 584, 595, 601, 607, 620, 634, 652, 653, 703, 715, 717, 720, 729, 745, 758, 764, 790, 804, 810, 829, 832, 851, 875, 879, 891, 896, 901, 925, 935, 938, 944, 945, 953, 958, 960, 985, 993, 999], 2: [22, 28, 30, 35, 39, 40, 68, 76, 80, 93, 105, 140, 148, 161, 186, 199, 201, 208, 211, 220, 224, 226, 233, 236, 237, 238, 239, 243, 245, 254, 272, 274, 276, 279, 284, 289, 297, 307, 352, 373, 384, 387, 401, 431, 438, 452, 455, 464, 465, 468, 469, 471, 475, 492, 495, 496, 501, 504, 520, 531, 572, 574, 581, 597, 599, 609, 613, 622, 638, 644, 650, 659, 670, 673, 679, 686, 689, 728, 731, 733, 738, 750, 751, 763, 785, 791, 794, 796, 818, 869, 873, 878, 884, 889, 890, 907, 911, 914, 916, 941, 949, 969, 971, 978, 988, 998], 3: [3, 6, 7, 21, 42, 53, 54, 72, 73, 83, 85, 94, 100, 102, 110, 114, 116, 123, 129, 130, 156, 167, 175, 177, 181, 197, 231, 234, 262, 268, 291, 304, 308, 316, 331, 336, 341, 381, 390, 397, 406, 424, 430, 437, 458, 477, 487, 488, 521, 522, 527, 530, 535, 551, 553, 554, 563, 610, 625, 627, 636, 639, 641, 642, 658, 675, 684, 688, 705, 706, 708, 710, 712, 721, 724, 727, 756, 771, 782, 787, 788, 792, 799, 800, 814, 820, 827, 833, 841, 857, 882, 886, 894, 908, 918, 939, 943, 952, 954, 959, 968, 981, 982, 990, 992, 996], 4: [23, 52, 86, 113, 115, 118, 121, 122, 133, 154, 169, 170, 183, 188, 193, 206, 228, 258, 259, 281, 285, 294, 310, 313, 317, 318, 323, 329, 347, 348, 353, 361, 363, 366, 380, 392, 402, 403, 409, 461, 484, 524, 525, 548, 559, 608, 612, 617, 635, 645, 655, 656, 665, 697, 707, 714, 716, 718, 726, 739, 754, 801, 807, 817, 824, 825, 836, 862, 867, 868, 871, 872, 902, 903, 933, 937, 942, 967, 974, 991], 5: [0, 8, 12, 13, 20, 25, 27, 36, 44, 46, 57, 60, 61, 88, 89, 90, 96, 101, 103, 109, 138, 141, 159, 172, 173, 182, 207, 216, 222, 223, 241, 246, 256, 260, 267, 302, 325, 334, 335, 338, 345, 364, 372, 376, 391, 418, 423, 446, 460, 466, 478, 479, 480, 489, 497, 499, 502, 507, 517, 533, 534, 549, 550, 571, 577, 582, 588, 615, 618, 624, 637, 640, 648, 660, 693, 722, 740, 752, 761, 767, 769, 773, 774, 775, 784, 798, 812, 837, 842, 843, 850, 864, 865, 895, 924, 926, 928, 936, 946, 956, 962, 977, 980, 987, 989, 994], 6: [16, 18, 32, 34, 37, 38, 48, 50, 56, 62, 65, 70, 75, 111, 134, 137, 142, 151, 185, 195, 202, 204, 209, 212, 213, 240, 293, 305, 319, 326, 328, 337, 355, 368, 379, 382, 383, 398, 422, 443, 444, 447, 474, 485, 508, 511, 545, 546, 547, 568, 569, 580, 583, 586, 587, 592, 598, 600, 616, 632, 647, 668, 683, 691, 694, 742, 746, 753, 762, 765, 783, 793, 821, 822, 826, 830, 834, 852, 893, 915, 927, 931, 947, 948, 965, 972], 7: [9, 43, 45, 63, 77, 91, 99, 108, 120, 125, 132, 144, 147, 149, 153, 157, 158, 178, 180, 184, 194, 203, 227, 230, 244, 248, 264, 265, 270, 273, 287, 290, 292, 295, 306, 332, 333, 342, 359, 367, 385, 388, 393, 399, 407, 410, 415, 416, 421, 425, 428, 442, 451, 456, 459, 463, 473, 481, 483, 498, 500, 505, 506, 512, 532, 558, 573, 590, 596, 604, 626, 649, 661, 662, 669, 676, 678, 682, 687, 690, 692, 704, 709, 711, 723, 748, 768, 772, 786, 809, 816, 823, 828, 831, 844, 853, 854, 861, 863, 877, 881, 905, 909, 912, 922, 923, 966, 975, 983, 995, 997], 8: [33, 41, 47, 49, 55, 69, 79, 95, 97, 117, 124, 127, 139, 145, 146, 150, 162, 164, 166, 176, 179, 187, 190, 196, 198, 251, 252, 275, 282, 283, 288, 298, 300, 311, 312, 314, 324, 327, 340, 343, 349, 351, 357, 358, 370, 394, 400, 404, 413, 414, 420, 427, 429, 448, 453, 454, 462, 503, 515, 519, 526, 536, 552, 555, 557, 576, 579, 591, 605, 614, 621, 630, 631, 646, 651, 663, 674, 681, 695, 699, 700, 702, 734, 735, 744, 747, 755, 770, 777, 779, 780, 806, 811, 815, 846, 847, 860, 870, 876, 883, 885, 892, 899, 900, 913, 917, 930, 932, 940, 951, 957, 961, 963, 964, 984], 9: [2, 4, 10, 24, 31, 64, 78, 82, 92, 98, 107, 126, 160, 174, 191, 192, 218, 219, 221, 225, 229, 247, 250, 253, 269, 271, 277, 286, 296, 299, 303, 315, 322, 346, 365, 374, 375, 378, 408, 412, 426, 432, 433, 440, 441, 449, 450, 472, 482, 493, 509, 518, 529, 539, 540, 541, 542, 544, 570, 575, 578, 589, 594, 602, 606, 611, 623, 628, 629, 633, 643, 654, 666, 677, 696, 701, 713, 725, 730, 732, 741, 749, 759, 766, 776, 789, 797, 803, 808, 813, 835, 838, 839, 845, 848, 849, 855, 856, 859, 887, 888, 897, 904, 910, 929, 934, 970, 973]}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Retrieve similar images for a given query\u001b[39;00m\n\u001b[0;32m      5\u001b[0m query_vector \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m70\u001b[39m) \u001b[38;5;66;03m# Query vector of dimension 70\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m similar_images \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39mretrieve(query_vector, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(similar_images)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(db\u001b[38;5;241m.\u001b[39mget_one_row(similar_images[\u001b[38;5;241m0\u001b[39m]))\n",
      "Cell \u001b[1;32mIn[57], line 93\u001b[0m, in \u001b[0;36mVecDB.retrieve\u001b[1;34m(self, query, top_k)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# # here we assume that the row number is the ID of each vector\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# for row_num in range(num_records):\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m#     vector = self.get_one_row(row_num)\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m#     score = self._cal_score(query, vector)\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m#     scores.append((score, row_num))\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# here we assume that if two rows have the same score, return the lowest ID\u001b[39;00m\n\u001b[0;32m     92\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(scores, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:top_k]\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [s[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m scores]\n",
      "Cell \u001b[1;32mIn[57], line 93\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# # here we assume that the row number is the ID of each vector\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# for row_num in range(num_records):\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m#     vector = self.get_one_row(row_num)\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m#     score = self._cal_score(query, vector)\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m#     scores.append((score, row_num))\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# here we assume that if two rows have the same score, return the lowest ID\u001b[39;00m\n\u001b[0;32m     92\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(scores, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:top_k]\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [s[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m scores]\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Create an instance of VecDB and random DB of size 10K\n",
    "db = VecDB(db_size = 10**3)\n",
    "\n",
    "# Retrieve similar images for a given query\n",
    "query_vector = np.random.rand(70) # Query vector of dimension 70\n",
    "similar_images = db.retrieve(query_vector, top_k=5)\n",
    "print(similar_images)\n",
    "\n",
    "print(db.get_one_row(similar_images[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8306092778089047\n",
      "0.8243485783299538\n",
      "0.8196922152617356\n",
      "0.8154010774167503\n",
      "0.8137924911889398\n"
     ]
    }
   ],
   "source": [
    "print(db._cal_score(query_vector,db.get_one_row((similar_images[0]))))\n",
    "print(db._cal_score(query_vector,db.get_one_row((similar_images[1]))))\n",
    "print(db._cal_score(query_vector,db.get_one_row((similar_images[2]))))\n",
    "print(db._cal_score(query_vector,db.get_one_row((similar_images[3]))))\n",
    "print(db._cal_score(query_vector,db.get_one_row((similar_images[4]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Score: 0.9746318461970762\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "db =VecDB(db_size=1000)\n",
    "query_vector = np.array([1, 2, 3])\n",
    "similar_image_vector = np.array([4, 5, 6])\n",
    "\n",
    "similarity = db._cal_score(query_vector, similar_image_vector)\n",
    "print(\"Cosine Similarity Score:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 2, 7, 4, 16, 18, 8, 10, 5, 1, 18, 7, 9, 13, 9, 15, 14, 2, 12, 13, 20, 4, 19, 16, 13, 13, 4, 15, 7, 19, 19, 18, 15, 15, 20, 18, 12, 14, 3, 8, 4, 14, 9, 12, 2, 19, 10, 15, 7, 2, 12, 6, 20, 4, 13, 17, 18, 17, 19, 3, 14, 19, 9, 7, 15, 13, 11, 19, 7, 15]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#consider this as a high dimensional vector\n",
    "vec = v = [random.randint(1,20) for i in range(70)]\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 2],\n",
       " [7, 4],\n",
       " [16, 18],\n",
       " [8, 10],\n",
       " [5, 1],\n",
       " [18, 7],\n",
       " [9, 13],\n",
       " [9, 15],\n",
       " [14, 2],\n",
       " [12, 13],\n",
       " [20, 4],\n",
       " [19, 16],\n",
       " [13, 13],\n",
       " [4, 15],\n",
       " [7, 19],\n",
       " [19, 18],\n",
       " [15, 15],\n",
       " [20, 18],\n",
       " [12, 14],\n",
       " [3, 8],\n",
       " [4, 14],\n",
       " [9, 12],\n",
       " [2, 19],\n",
       " [10, 15],\n",
       " [7, 2],\n",
       " [12, 6],\n",
       " [20, 4],\n",
       " [13, 17],\n",
       " [18, 17],\n",
       " [19, 3],\n",
       " [14, 19],\n",
       " [9, 7],\n",
       " [15, 13],\n",
       " [11, 19],\n",
       " [7, 15]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_count = 35\n",
    "vector_size = len(vec)\n",
    "\n",
    "# vector_size must be divisable by chunk_size\n",
    "assert vector_size % chunk_count == 0\n",
    "# length of each subvector will be vector_size/ chunk_count\n",
    "subvector_size = int(vector_size / chunk_count)\n",
    "\n",
    "# subvectors\n",
    "sub_vectors = [vec[row: row+subvector_size] for row in range(0, vector_size, subvector_size)]\n",
    "sub_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# subvectors are then processed and linked to their closest centroids, also known as reproduction values, within the respective subclusters.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m56\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m k \u001b[38;5;241m%\u001b[39m chunk_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m k_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(k\u001b[38;5;241m/\u001b[39mchunk_count)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m randint\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# subvectors are then processed and linked to their closest centroids, also known as reproduction values, within the respective subclusters.\n",
    "\n",
    "k = 56\n",
    "assert k % chunk_count == 0\n",
    "k_ = int(k/chunk_count)\n",
    "\n",
    "from random import randint\n",
    "# reproduction values\n",
    "c = []  \n",
    "for j in range(chunk_count):\n",
    "    # each j represents a subvector position\n",
    "    c_j = []\n",
    "    for i in range(k_):\n",
    "        # each i represents a cluster/reproduction value position \n",
    "       c_ji = [randint(0, 9) for _ in range(subvector_size)]\n",
    "       c_j.append(c_ji)  # add cluster centroid to subspace list\n",
    "    \n",
    "  # add subspace list of centroids\n",
    "    c.append(c_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to calculate euclidean distance\n",
    "def euclidean(v, u):\n",
    "    distance = sum((x - y) ** 2 for x, y in zip(v, u)) ** .5\n",
    "    return distance\n",
    "\n",
    "#helper function to create unique ids\n",
    "def nearest(c_j, chunk_j):\n",
    "    distance = 9e9\n",
    "    for i in range(k_):\n",
    "        new_dist = euclidean(c_j[i], chunk_j)\n",
    "        if new_dist < distance:\n",
    "            nearest_idx = i\n",
    "            distance = new_dist\n",
    "    return nearest_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 2, 0, 2, 0, 0, 3, 0, 1, 3, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "ids = []\n",
    "# unique centroid IDs for each subvector\n",
    "for j in range(chunk_count):\n",
    "    i = nearest(c[j], sub_vectors[j])\n",
    "    ids.append(i)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 4, 0, 2, 9, 7, 4, 7, 1, 8, 3, 0, 9, 8, 7, 7, 8, 7, 4, 3, 5, 9, 9, 7, 5, 5, 7, 7, 8, 4, 6, 9, 5, 4, 9, 9, 0, 9, 0, 8, 0, 8, 8, 9, 3, 5, 6, 4, 3, 3, 6, 9, 4, 2, 4, 8, 6, 8, 5, 9, 6, 8, 9, 9, 0, 0, 4, 9, 5, 9]\n"
     ]
    }
   ],
   "source": [
    "quantized = []\n",
    "for j in range(chunk_count):\n",
    "    c_ji = c[j][ids[j]]\n",
    "    quantized.extend(c_ji)\n",
    "\n",
    "print(quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6 1 2 9 3 6 4]\n",
      " [2 9 6 6 1 1 8]\n",
      " [6 8 0 4 3 9 3]\n",
      " [5 5 4 8 8 1 7]\n",
      " [2 2 4 7 3 8 4]\n",
      " [8 2 9 7 9 1 5]\n",
      " [0 7 9 4 3 1 5]\n",
      " [6 3 4 2 0 5 0]\n",
      " [8 4 9 2 0 6 5]\n",
      " [5 4 1 3 0 2 9]]\n",
      "Cluster Centers: [[3.75       5.75       7.         6.25       5.25       1.\n",
      "  6.25      ]\n",
      " [5.5        3.66666667 3.33333333 4.5        1.5        6.\n",
      "  4.16666667]]\n",
      "Labels: [1 0 1 0 1 0 0 1 1 1]\n",
      "Posting Lists: {0: [1, 3, 5, 6], 1: [0, 2, 4, 7, 8, 9]}\n",
      "Actual vector: [[6 6 6 5 8 1 4]]\n",
      "Nearest vector: [5 5 4 8 8 1 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "vectors = np.random.randint(0, 10, (10, 7))  # Integers between 0 and 9\n",
    "\n",
    "print(vectors)\n",
    "# Step 1: Coarse Quantization (Clustering)\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "labels = kmeans.fit_predict(vectors)  # Assign each vector to a cluster\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "# Step 2: Construct Posting Lists\n",
    "posting_lists = {i: [] for i in range(2)}  # Two clusters: 0 and 1\n",
    "for i, label in enumerate(labels):\n",
    "    posting_lists[label].append(i)\n",
    "    \n",
    "\n",
    "# Print Clustering Results\n",
    "print(\"Cluster Centers:\", cluster_centers)\n",
    "print(\"Labels:\", labels)\n",
    "print(\"Posting Lists:\", posting_lists)\n",
    "\n",
    "# Query Processing\n",
    "query = np.random.randint(0, 10, (1, 7))  # Integers between 0 and 9\n",
    "nearest_centroid = np.argmin([np.linalg.norm(query - centroid) for centroid in kmeans.cluster_centers_])\n",
    "\n",
    "# Fine Search (Within the posting list of nearest centroid)\n",
    "nearest_vectors = [vectors[i] for i in posting_lists[nearest_centroid]]\n",
    "closest_vector = min(nearest_vectors, key=lambda x: np.linalg.norm(query - x))\n",
    "print(f\"Actual vector: {query}\")\n",
    "print(f\"Nearest vector: {closest_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.94669777 0.14702176 0.03122288 0.93221853 0.8269393  0.78845497\n",
      "  0.30489522 0.9944374  0.97145154 0.72643284 0.11840847 0.83041497\n",
      "  0.25410993 0.29975197 0.16046107 0.2680251 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09518839 0.35055304 0.9398079  ... 0.95536698 0.62127548 0.25900692]\n",
      " [0.60531046 0.0299943  0.18020337 ... 0.84442837 0.30599367 0.31717393]\n",
      " [0.12436448 0.70804765 0.40053623 ... 0.01026024 0.98808191 0.81107594]\n",
      " ...\n",
      " [0.83905272 0.73320902 0.38375223 ... 0.27363777 0.37699509 0.06085107]\n",
      " [0.06235347 0.81673454 0.83788639 ... 0.13500675 0.61006013 0.20705075]\n",
      " [0.5731672  0.76549188 0.87305659 ... 0.68015527 0.97821466 0.63104061]]\n",
      "[[ 55 168  80 ...  63  69 117]\n",
      " [157 240 141 ... 103 160 169]\n",
      " [111 134  58 ...  23  49   5]\n",
      " ...\n",
      " [227  34 191 ... 211 147  18]\n",
      " [ 77  66 245 ... 150 134 118]\n",
      " [ 51 120  81 ... 228 224  70]]\n",
      "[[0.09794463 0.35227004 0.91964504 ... 0.95014297 0.62753523 0.24410977]\n",
      " [0.61093234 0.01673905 0.15575159 ... 0.85667647 0.30498966 0.29733311]\n",
      " [0.12194592 0.71991971 0.39669213 ... 0.01502664 0.98060759 0.81912355]\n",
      " ...\n",
      " [0.82988276 0.71941122 0.39528657 ... 0.29586202 0.37226255 0.06036116]\n",
      " [0.07032296 0.81166198 0.84902497 ... 0.12305148 0.57763475 0.18521893]\n",
      " [0.54778534 0.75887545 0.88783776 ... 0.6724173  0.98195663 0.64319453]]\n",
      "Nearest neighbors: [244 940 274 305 863]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class ProductQuantization:\n",
    "    def __init__(self, m, k):\n",
    "        \"\"\"\n",
    "        Initialize the Product Quantization class.\n",
    "        :param m: Number of subspaces (split vector into m parts)\n",
    "        :param k: Number of clusters per subspace\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.k = k\n",
    "        self.codebooks = []\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        Fit the quantizer to the data.\n",
    "        :param data: Dataset of shape (n_samples, d_features)\n",
    "        \"\"\"\n",
    "        n_samples, d_features = data.shape\n",
    "        assert d_features % self.m == 0, \"Number of features must be divisible by m\"\n",
    "        \n",
    "        self.subvector_dim = d_features // self.m\n",
    "        self.codebooks = []\n",
    "        \n",
    "        for i in range(self.m):\n",
    "            sub_data = data[:, i * self.subvector_dim:(i + 1) * self.subvector_dim]\n",
    "            kmeans = KMeans(n_clusters=self.k, random_state=42).fit(sub_data)\n",
    "            self.codebooks.append(kmeans.cluster_centers_)\n",
    "    \n",
    "    def encode(self, data): #build the index\n",
    "        \"\"\"\n",
    "        Encode the dataset into quantized indices.\n",
    "        :param data: Dataset of shape (n_samples, d_features)\n",
    "        :return: Encoded indices of shape (n_samples, m)\n",
    "        \"\"\"\n",
    "        n_samples, d_features = data.shape\n",
    "        codes = np.zeros((n_samples, self.m), dtype=np.int32) # Stores the nearest centroid for each subspace: if m = 4 -> [[1,5,1,6],[9,1,5,3],...]\n",
    "        \n",
    "        for i in range(self.m):\n",
    "            sub_data = data[:, i * self.subvector_dim:(i + 1) * self.subvector_dim] #partition the data into subspaces\n",
    "            distances = cdist(sub_data, self.codebooks[i]) #\n",
    "            codes[:, i] = np.argmin(distances, axis=1)\n",
    "        \n",
    "        return codes #codes contain the compressed representation of the data\n",
    "    \n",
    "    def decode(self, codes): #used \n",
    "        \"\"\"\n",
    "        Decode the quantized indices back to approximate vectors.\n",
    "        :param codes: Encoded indices of shape (n_samples, m)\n",
    "        :return: Approximate vectors of shape (n_samples, d_features)\n",
    "        \"\"\"\n",
    "        n_samples = codes.shape[0]\n",
    "        decoded_vectors = np.zeros((n_samples, self.m * self.subvector_dim))\n",
    "        \n",
    "        for i in range(self.m):\n",
    "            decoded_vectors[:, i * self.subvector_dim:(i + 1) * self.subvector_dim] = self.codebooks[i][codes[:, i]]\n",
    "        \n",
    "        return decoded_vectors\n",
    "\n",
    "    def search(self, query, codes, top_k=1):\n",
    "        \"\"\"\n",
    "        Perform approximate nearest neighbor search.\n",
    "        :param query: Query vector of shape (1, d_features)\n",
    "        :param codes: Encoded indices of the dataset\n",
    "        :param top_k: Number of nearest neighbors to return\n",
    "        :return: Indices of top_k nearest neighbors\n",
    "        \"\"\"\n",
    "        distances = np.zeros(codes.shape[0])\n",
    "        \n",
    "        for i in range(self.m):\n",
    "            sub_query = query[:, i * self.subvector_dim:(i + 1) * self.subvector_dim]\n",
    "            sub_distances = cdist(sub_query, self.codebooks[i])\n",
    "            distances += sub_distances[0, codes[:, i]]\n",
    "        \n",
    "        return np.argsort(distances)[:top_k]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data\n",
    "    data = np.random.random((1000, 16))  # 1000 samples, 16 dimensions\n",
    "    \n",
    "    query = np.random.random((1, 16))   # 1 query vector\n",
    "    print(query)\n",
    "    pq = ProductQuantization(m=8, k=256)  # 4 subspaces, 256 clusters per subspace\n",
    "    pq.fit(data)\n",
    "    \n",
    "    codes = pq.encode(data)\n",
    "    reconstructed_data = pq.decode(codes)\n",
    "    \n",
    "    # Perform search\n",
    "    neighbors = pq.search(query, codes, top_k=5)\n",
    "    print(data)\n",
    "    print(codes)\n",
    "    print(reconstructed_data)\n",
    "    print(\"Nearest neighbors:\", neighbors)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

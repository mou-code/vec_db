{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Annotated\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "DB_SEED_NUMBER = 42\n",
    "ELEMENT_SIZE = np.dtype(np.float32).itemsize\n",
    "DIMENSION = 70\n",
    "class CustomIndex: pass\n",
    "\n",
    "class VecDB:\n",
    "    def __init__(self, database_file_path = \"saved_db.dat\", index_file_path = \"index.dat\", new_db = True, db_size = None) -> None:\n",
    "        self.db_path = database_file_path\n",
    "        self.index_path = index_file_path\n",
    "        if new_db:\n",
    "            if db_size is None:\n",
    "                raise ValueError(\"You need to provide the size of the database\")\n",
    "            # delete the old DB file if exists\n",
    "            if os.path.exists(self.db_path):\n",
    "                os.remove(self.db_path)\n",
    "            self.generate_database(db_size)\n",
    "    \n",
    "    def generate_database(self, size: int) -> None:\n",
    "        rng = np.random.default_rng(DB_SEED_NUMBER)\n",
    "        vectors = rng.random((size, DIMENSION), dtype=np.float32)\n",
    "        self._write_vectors_to_file(vectors)\n",
    "        self._build_index()\n",
    "    # np.memmap() Parameters:\n",
    "    # First argument: File path to store data\n",
    "    # dtype: Data type (float32 here)\n",
    "    # mode: File access mode\n",
    "    # 'w+': Read/write, create if not exists\n",
    "    # shape: Dimensions of the array\n",
    "    # returns file contents on disk\n",
    "    def _write_vectors_to_file(self, vectors: np.ndarray) -> None:\n",
    "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='w+', shape=vectors.shape)\n",
    "        mmap_vectors[:] = vectors[:]\n",
    "        mmap_vectors.flush()\n",
    "\n",
    "    def _get_num_records(self) -> int:\n",
    "        return os.path.getsize(self.db_path) // (DIMENSION * ELEMENT_SIZE)\n",
    "\n",
    "    def insert_records(self, rows: Annotated[np.ndarray, (int, 70)]):\n",
    "        num_old_records = self._get_num_records()\n",
    "        num_new_records = len(rows)\n",
    "        full_shape = (num_old_records + num_new_records, DIMENSION)\n",
    "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='r+', shape=full_shape)\n",
    "        mmap_vectors[num_old_records:] = rows\n",
    "        mmap_vectors.flush()\n",
    "        #TODO: might change to call insert in the index, if you need\n",
    "        self._build_index()\n",
    "\n",
    "    def get_one_row(self, row_num: int) -> np.ndarray:\n",
    "        # This function is only load one row in memory\n",
    "        try:\n",
    "            offset = row_num * DIMENSION * ELEMENT_SIZE\n",
    "            # [0] is necessary because:\n",
    "            # mmap_vector is 2D array: [[x1, x2, ..., x70]]\n",
    "            # [0] extracts the single row as 1D: [x1, x2, ..., x70]\n",
    "            mmap_vector = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(1, DIMENSION), offset=offset)\n",
    "            return np.array(mmap_vector[0])\n",
    "        except Exception as e:\n",
    "            return f\"An error occurred: {e}\"\n",
    "\n",
    "    def get_all_rows(self) -> np.ndarray:\n",
    "        # Take care this load all the data in memory\n",
    "        num_records = self._get_num_records()\n",
    "        vectors = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(num_records, DIMENSION))\n",
    "        return np.array(vectors)\n",
    "    \n",
    "    def retrieve(self, query: Annotated[np.ndarray, (1, DIMENSION)], top_k = 5):\n",
    "        scores = []\n",
    "        query = query.ravel()  # Flattens the query to 1D\n",
    "        num_records = self._get_num_records()\n",
    "        file = open(self.index_path,'rb')\n",
    "        index = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "        cluster_centers=index.centroids\n",
    "        labels_list=index.labels_list\n",
    "        for i,vec in enumerate(cluster_centers):\n",
    "            score=self._cal_score(query,vec)\n",
    "            scores.append((score,i))\n",
    "        # # here we assume that the row number is the ID of each vector\n",
    "        # for row_num in range(num_records):\n",
    "        #     vector = self.get_one_row(row_num)\n",
    "        #     score = self._cal_score(query, vector)\n",
    "        #     scores.append((score, row_num))\n",
    "        # here we assume that if two rows have the same score, return the lowest ID\n",
    "        # Getting the nearest clusters\n",
    "        n_probe = 5\n",
    "        cluster_scores = sorted(scores, reverse=True)[:n_probe]\n",
    "        # Get the vectors of nearest clusters\n",
    "        top_vector=[]\n",
    "        # for item in scores:\n",
    "        top_vector.append(labels_list[cluster_scores[0][1]])\n",
    "        # print(\"top_vector\",top_vector[0])\n",
    "        \n",
    "        # store the vectors of the top cluster\n",
    "        resulted_vectors=[]\n",
    "        #loop over top_vectors and cosine similarity\n",
    "        for row_num in top_vector[0]:\n",
    "            vector = self.get_one_row(row_num)\n",
    "            vector = vector.ravel()  # Flattens the vector to 1D\n",
    "            score = self._cal_score(query, vector)\n",
    "            resulted_vectors.append((score,row_num))\n",
    "        # Sort by scores and keep only top_k results\n",
    "        resulted_vectors = sorted(resulted_vectors, reverse=True)[:top_k]\n",
    "        # print(resulted_vectors)\n",
    "        # Extract only the row_num from resulted_vectors\n",
    "        row_nums = [row_num for _, row_num in resulted_vectors]\n",
    "\n",
    "        return row_nums  # Return only row_num values\n",
    "    \n",
    "    def _cal_score(self, vec1, vec2):\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm_vec1 = np.linalg.norm(vec1)\n",
    "        norm_vec2 = np.linalg.norm(vec2)\n",
    "        cosine_similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "        return cosine_similarity\n",
    "\n",
    "    def _build_index(self, num_clusters=100, num_subspaces=7):\n",
    "            \n",
    "            # Placeholder for index building logic\n",
    "            ### IVF\n",
    "            # Apply k means clustering to all vectors -> return the cluster centroids \n",
    "            # store the clusters and their centroids in a array or dictionary?\n",
    "            # Within each cluster:\n",
    "            # loop over each vector \n",
    "            # create an array of subspace arrays (parameter) subspaces[[   [subvector1 of array 1],[subvector1 of array 2]   ] , [],...]\n",
    "            # apply k-means clustering for each subspace[0] subspace[1] etc..\n",
    "            # Generate the codebook\n",
    "            \n",
    "            \n",
    "            # Build the PQ-IVF index.\n",
    "            # Args:\n",
    "            #     num_clusters (int): Number of clusters for the inverted file (IVF).\n",
    "            #     num_subspaces (int): Number of subspaces for product quantization.\n",
    "            #     subspace_dim (int): Dimension of each subspace.\n",
    "            # Sample dataset\n",
    "            vectors = self.get_all_rows()\n",
    "\n",
    "            # Step 1: Coarse Quantization (Clustering)\n",
    "            n_clusters=200\n",
    "            kmeans = KMeans(n_clusters)\n",
    "            labels = kmeans.fit_predict(vectors)  # Assign each vector to a cluster\n",
    "            cluster_centers = kmeans.cluster_centers_\n",
    "            # Step 2: Construct Posting Lists\n",
    "            labels_list = {i: [] for i in range(n_clusters)}  # Two clusters: 0 and 1\n",
    "            for i, label in enumerate(labels):\n",
    "                labels_list[label].append(i)\n",
    "            \n",
    "            CustomIndex.centroids=cluster_centers\n",
    "            CustomIndex.labels_list=labels_list\n",
    "            \n",
    "            filehandler = open(self.index_path,\"wb\")\n",
    "            pickle.dump(CustomIndex,filehandler)\n",
    "            filehandler.close()\n",
    "\n",
    "            # print(object_file.centroids, object_file.labels_list, sep=', ')\n",
    "            # Print Clustering Results\n",
    "            # print(\"Cluster Centers:\", cluster_centers)\n",
    "            # print(\"Labels:\", labels)\n",
    "            # print(\"Posting Lists:\", labels_list)\n",
    "\n",
    "            # # # Query Processing\n",
    "            # query = np.random.randint(0, 10, (1, 7))  # Integers between 0 and 9\n",
    "            # nearest_centroid = np.argmin([np.linalg.norm(query - centroid) for centroid in kmeans.cluster_centers_])\n",
    "\n",
    "            # # Fine Search (Within the posting list of nearest centroid)\n",
    "            # nearest_vectors = [vectors[i] for i in labels_list[nearest_centroid]]\n",
    "            # closest_vector = min(nearest_vectors, key=lambda x: np.linalg.norm(query - x))\n",
    "            # print(f\"Actual vector: {query}\")\n",
    "            # print(f\"Nearest vector: {closest_vector}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of VecDB and random DB of size 10K\n",
    "db = VecDB(db_size = 10**7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4019341, 2539407, 4823272, 1342913, 5605117]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Retrieve similar images for a given query\n",
    "query_vector = np.random.rand(70) # Query vector of dimension 70\n",
    "similar_images = db.retrieve(query_vector, top_k=5)\n",
    "print(similar_images)\n",
    "# print(db.get_one_row((similar_images[0])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9078978664318471\n",
      "0.9000237664630433\n",
      "0.8948934506872064\n",
      "0.8940165329832732\n",
      "0.893875506043859\n"
     ]
    }
   ],
   "source": [
    "print(db._cal_score(query_vector,db.get_one_row((similar_images[0]))))\n",
    "print(db._cal_score(query_vector,db.get_one_row((similar_images[1]))))\n",
    "print(db._cal_score(query_vector,db.get_one_row((similar_images[2]))))\n",
    "print(db._cal_score(query_vector,db.get_one_row((similar_images[3]))))\n",
    "print(db._cal_score(query_vector,db.get_one_row((similar_images[4]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Score: 0.9746318461970762\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "db =VecDB(db_size=1000)\n",
    "query_vector = np.array([1, 2, 3])\n",
    "similar_image_vector = np.array([4, 5, 6])\n",
    "\n",
    "similarity = db._cal_score(query_vector, similar_image_vector)\n",
    "print(\"Cosine Similarity Score:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 4, 20, 16, 16, 1, 4, 6, 14, 12, 1, 7, 19, 1, 7, 9, 2, 14, 4, 10, 3, 8, 12, 4, 19, 12, 10, 18, 6, 4, 1, 2, 17, 17, 5, 7, 8, 16, 17, 20, 11, 12, 4, 17, 14, 4, 15, 6, 12, 20, 16, 2, 2, 5, 10, 1, 8, 6, 3, 3, 7, 4, 14, 2, 2, 16, 7, 17, 2]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#consider this as a high dimensional vector\n",
    "vec = v = [random.randint(1,20) for i in range(70)]\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 2],\n",
       " [4, 20],\n",
       " [16, 16],\n",
       " [1, 4],\n",
       " [6, 14],\n",
       " [12, 1],\n",
       " [7, 19],\n",
       " [1, 7],\n",
       " [9, 2],\n",
       " [14, 4],\n",
       " [10, 3],\n",
       " [8, 12],\n",
       " [4, 19],\n",
       " [12, 10],\n",
       " [18, 6],\n",
       " [4, 1],\n",
       " [2, 17],\n",
       " [17, 5],\n",
       " [7, 8],\n",
       " [16, 17],\n",
       " [20, 11],\n",
       " [12, 4],\n",
       " [17, 14],\n",
       " [4, 15],\n",
       " [6, 12],\n",
       " [20, 16],\n",
       " [2, 2],\n",
       " [5, 10],\n",
       " [1, 8],\n",
       " [6, 3],\n",
       " [3, 7],\n",
       " [4, 14],\n",
       " [2, 2],\n",
       " [16, 7],\n",
       " [17, 2]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_count = 35\n",
    "vector_size = len(vec)\n",
    "\n",
    "# vector_size must be divisable by chunk_size\n",
    "assert vector_size % chunk_count == 0\n",
    "# length of each subvector will be vector_size/ chunk_count\n",
    "subvector_size = int(vector_size / chunk_count)\n",
    "\n",
    "# subvectors\n",
    "sub_vectors = [vec[row: row+subvector_size] for row in range(0, vector_size, subvector_size)]\n",
    "sub_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# subvectors are then processed and linked to their closest centroids, also known as reproduction values, within the respective subclusters.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m56\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m k \u001b[38;5;241m%\u001b[39m chunk_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m k_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(k\u001b[38;5;241m/\u001b[39mchunk_count)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m randint\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# subvectors are then processed and linked to their closest centroids, also known as reproduction values, within the respective subclusters.\n",
    "\n",
    "k = 56\n",
    "assert k % chunk_count == 0\n",
    "k_ = int(k/chunk_count)\n",
    "\n",
    "from random import randint\n",
    "# reproduction values\n",
    "c = []  \n",
    "for j in range(chunk_count):\n",
    "    # each j represents a subvector position\n",
    "    c_j = []\n",
    "    for i in range(k_):\n",
    "        # each i represents a cluster/reproduction value position \n",
    "       c_ji = [randint(0, 9) for _ in range(subvector_size)]\n",
    "       c_j.append(c_ji)  # add cluster centroid to subspace list\n",
    "    \n",
    "  # add subspace list of centroids\n",
    "    c.append(c_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to calculate euclidean distance\n",
    "def euclidean(v, u):\n",
    "    distance = sum((x - y) ** 2 for x, y in zip(v, u)) ** .5\n",
    "    return distance\n",
    "\n",
    "#helper function to create unique ids\n",
    "def nearest(c_j, chunk_j):\n",
    "    distance = 9e9\n",
    "    for i in range(k_):\n",
    "        new_dist = euclidean(c_j[i], chunk_j)\n",
    "        if new_dist < distance:\n",
    "            nearest_idx = i\n",
    "            distance = new_dist\n",
    "    return nearest_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 2, 0, 2, 0, 0, 3, 0, 1, 3, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "ids = []\n",
    "# unique centroid IDs for each subvector\n",
    "for j in range(chunk_count):\n",
    "    i = nearest(c[j], sub_vectors[j])\n",
    "    ids.append(i)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 4, 0, 2, 9, 7, 4, 7, 1, 8, 3, 0, 9, 8, 7, 7, 8, 7, 4, 3, 5, 9, 9, 7, 5, 5, 7, 7, 8, 4, 6, 9, 5, 4, 9, 9, 0, 9, 0, 8, 0, 8, 8, 9, 3, 5, 6, 4, 3, 3, 6, 9, 4, 2, 4, 8, 6, 8, 5, 9, 6, 8, 9, 9, 0, 0, 4, 9, 5, 9]\n"
     ]
    }
   ],
   "source": [
    "quantized = []\n",
    "for j in range(chunk_count):\n",
    "    c_ji = c[j][ids[j]]\n",
    "    quantized.extend(c_ji)\n",
    "\n",
    "print(quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[77 43 16 44 93 15 96]\n",
      " [86 37 88 34 71 63 87]\n",
      " [19 98 36 14 69 61 63]\n",
      " [88 89 39  3 77 18 97]\n",
      " [78 86 71 68 44 29 76]\n",
      " [26  8 91 13 68 88 82]\n",
      " [93 48 97  1  3 35 17]\n",
      " [ 9 74 93 79  8 66 53]\n",
      " [40 38 77 30 58 89 49]\n",
      " [94 95 51 96 84 97 26]]\n",
      "Cluster Centers: [[81.         72.66666667 42.         38.33333333 71.33333333 20.66666667\n",
      "  89.66666667]\n",
      " [52.42857143 56.85714286 76.14285714 38.14285714 51.57142857 71.28571429\n",
      "  53.85714286]]\n",
      "Labels: [0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 1\n",
      " 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0\n",
      " 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1]\n",
      "Posting Lists: {0: [0, 3, 4, 10, 12, 13, 14, 19, 21, 23, 24, 29, 30, 32, 35, 41, 44, 49, 52, 54, 63, 68, 69, 73, 75, 77, 80, 88, 90, 92, 93], 1: [1, 2, 5, 6, 7, 8, 9, 11, 15, 16, 17, 18, 20, 22, 25, 26, 27, 28, 31, 33, 34, 36, 37, 38, 39, 40, 42, 43, 45, 46, 47, 48, 50, 51, 53, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 70, 71, 72, 74, 76, 78, 79, 81, 82, 83, 84, 85, 86, 87, 89, 91, 94, 95, 96, 97, 98, 99]}\n",
      "Actual vector: [[9 0 6 2 6 6 8]]\n",
      "Nearest vector: [30 40 25  8  3 55 39]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "vectors = np.random.randint(0, 100, (100, 7))  # Integers between 0 and 9\n",
    "\n",
    "# print(vectors)\n",
    "# Step 1: Coarse Quantization (Clustering)\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "print(vectors[:10])\n",
    "kmeans.fit(vectors[:10])\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "labels = kmeans.predict(vectors)  # Assign each vector to a cluster\n",
    "\n",
    "# Step 2: Construct Posting Lists\n",
    "posting_lists = {i: [] for i in range(2)}  # Two clusters: 0 and 1\n",
    "for i, label in enumerate(labels):\n",
    "    posting_lists[label].append(i)\n",
    "    \n",
    "\n",
    "# Print Clustering Results\n",
    "print(\"Cluster Centers:\", cluster_centers)\n",
    "print(\"Labels:\", labels)\n",
    "print(\"Posting Lists:\", posting_lists)\n",
    "\n",
    "# Query Processing\n",
    "query = np.random.randint(0, 10, (1, 7))  # Integers between 0 and 9\n",
    "nearest_centroid = np.argmin([np.linalg.norm(query - centroid) for centroid in kmeans.cluster_centers_])\n",
    "\n",
    "# Fine Search (Within the posting list of nearest centroid)\n",
    "nearest_vectors = [vectors[i] for i in posting_lists[nearest_centroid]]\n",
    "closest_vector = min(nearest_vectors, key=lambda x: np.linalg.norm(query - x))\n",
    "print(f\"Actual vector: {query}\")\n",
    "print(f\"Nearest vector: {closest_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labelsp1 [2 2 0 0 0 1 0 0 1 1 1 2 0 1 2 0 1 1 1 0 2 2 1 0 1 1 2 0 1 0 2 1 1 1 2 0 2\n",
      " 0 0 1 2 0 1 0 2 2 0 0 1 2 1 1 2 2 2 2 0 0 2 2 2 2 1 2 2 2 0 2 2 1 1 2 0 1\n",
      " 2 0 2 1 1 1 0 1 0 0 0 1 1 1 1 0 2 1 2 2 2 1 2 0 1 0]\n",
      "labelsp2 [2 2 2 0 2 1 1 2 1 0 2 2 0 1 1 0 1 2 2 0 1 1 0 0 1 0 2 2 1 0 2 1 0 2 0 2 1\n",
      " 0 2 0 2 1 0 2 1 1 1 0 1 2 2 1 0 2 2 2 2 1 2 0 2 1 1 0 0 1 1 0 2 2 2 0 2 1\n",
      " 2 2 2 1 2 2 1 2 2 2 2 2 2 1 2 1 2 1 2 1 1 1 1 2 2 0]\n",
      "Cluster Pair (0, 0): [3, 12, 15, 19, 23, 29, 37, 47, 99]\n",
      "Cluster Pair (0, 1): [6, 41, 46, 57, 66, 80, 89]\n",
      "Cluster Pair (0, 2): [2, 4, 7, 27, 35, 38, 43, 56, 72, 75, 82, 83, 84, 97]\n",
      "Cluster Pair (1, 0): [9, 22, 25, 32, 39, 42]\n",
      "Cluster Pair (1, 1): [5, 8, 13, 16, 24, 28, 31, 48, 51, 62, 73, 77, 87, 91, 95]\n",
      "Cluster Pair (1, 2): [10, 17, 18, 33, 50, 69, 70, 78, 79, 81, 85, 86, 88, 98]\n",
      "Cluster Pair (2, 0): [34, 52, 59, 63, 64, 67, 71]\n",
      "Cluster Pair (2, 1): [14, 20, 21, 36, 44, 45, 61, 65, 93, 94, 96]\n",
      "Cluster Pair (2, 2): [0, 1, 11, 26, 30, 40, 49, 53, 54, 55, 58, 60, 68, 74, 76, 90, 92]\n",
      "[[12 19 99 91  5 54 85 29  9 33]]\n",
      "Nearest U Indices =  [0 2 1]  Nearest V indices =  [2 0 1]\n",
      "Nearest u,v: [(198.42045112785925, (1, 1)), (189.06436868290675, (1, 0)), (171.1650305791626, (2, 1)), (167.69580121722004, (1, 2)), (167.43165040998372, (0, 1)), (161.80894813421008, (2, 0)), (158.0755679650312, (0, 0)), (140.44038066852337, (2, 2)), (136.7070004993445, (0, 2))]\n",
      "Nearest vectors: [5, 8, 13, 16, 24, 28, 31, 48, 51, 62, 73, 77, 87, 91, 95]\n",
      "(167.43165040998372, (0, 1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "vectors = np.random.randint(0, 100, (100, 10))  # Integers between 0 and 9\n",
    "# print(vectors)\n",
    "part1 = vectors[:,:5]\n",
    "part2 = vectors[:,5:]\n",
    "# print(part1)\n",
    "# print(part2)\n",
    "n_clusters = 3\n",
    "# print(vectors)\n",
    "# Step 1: Coarse Quantization (Clustering)\n",
    "kmeansp1 = KMeans(n_clusters)\n",
    "kmeansp1.fit(part1)\n",
    "cluster_centersp1 = kmeansp1.cluster_centers_\n",
    "\n",
    "kmeansp2 = KMeans(n_clusters)\n",
    "kmeansp2.fit(part2)\n",
    "cluster_centersp2 = kmeansp2.cluster_centers_\n",
    "\n",
    "\n",
    "\n",
    "labelsp1 = kmeansp1.predict(part1)  # Assign each vector's part1 to a cluster\n",
    "labelsp2 = kmeansp2.predict(part2)  # Assign each vector's part2 to a cluster\n",
    "\n",
    "print(\"labelsp1\",labelsp1)\n",
    "print(\"labelsp2\",labelsp2)\n",
    "\n",
    "# Step 2: Construct Posting Lists for Cartesian Product of Clusters\n",
    "# Initialize posting lists for all cluster pairs\n",
    "posting_lists = {(i, j): [] for i in range(n_clusters) for j in range(n_clusters)}\n",
    "\n",
    "# Assign each vector to the corresponding posting list\n",
    "for idx, (label1, label2) in enumerate(zip(labelsp1, labelsp2)):\n",
    "    posting_lists[(label1, label2)].append(idx)\n",
    "\n",
    "# Display the posting lists\n",
    "for key, indices in posting_lists.items():\n",
    "    print(f\"Cluster Pair {key}: {indices}\")\n",
    "    \n",
    "\n",
    "# # Print Clustering Results\n",
    "# print(\"Cluster Centers:\", cluster_centers)\n",
    "# print(\"Labels:\", labels)\n",
    "# print(\"Posting Lists:\", posting_lists)\n",
    "\n",
    "# # Query Processing\n",
    "query = np.random.randint(0, 100, (1, 10))  # Integers between 0 and 9\n",
    "print(query)\n",
    "query_part1 = query[:,:5]\n",
    "query_part2 = query[:,5:]\n",
    "k=5\n",
    "# Compute distances to all u's (centroids of part1)\n",
    "#calculate distance\n",
    "# distances_u = kmeansp1.predict(query_part1)\n",
    "distances_u = [np.linalg.norm(query_part1 - centroid) for centroid in kmeansp1.cluster_centers_]\n",
    "\n",
    "nearest_u_indices = np.argsort(distances_u)[:k]  # Indices of top-k closest u's\n",
    "# Compute distances to all v's (centroids of part2)\n",
    "# distances_v = kmeansp2.predict(query_part2)\n",
    "distances_v = [np.linalg.norm(query_part2 - centroid) for centroid in kmeansp2.cluster_centers_]\n",
    "\n",
    "nearest_v_indices = np.argsort(distances_v)[:k]  # Indices of top-k closest v's\n",
    "print (\"Nearest U Indices = \",nearest_u_indices,\" Nearest V indices = \", nearest_v_indices)\n",
    "\n",
    "# Combine u's and v's and find the pair with the smallest combined distance\n",
    "min_distance = float('inf')\n",
    "best_pairs = []\n",
    "for u_idx in nearest_u_indices:\n",
    "    for v_idx in nearest_v_indices:\n",
    "        combined_distance = distances_u[u_idx] + distances_v[v_idx]\n",
    "        # if combined_distance < min_distance:\n",
    "        best_pairs.append((combined_distance,(u_idx,v_idx)))\n",
    "        # best_pairs.append((u_idx,v_idx))\n",
    "best_pairs = sorted(best_pairs, reverse=True)\n",
    "\n",
    "\n",
    "print(f\"Nearest u,v: {best_pairs}\")\n",
    "print(f\"Nearest vectors: {posting_lists[best_pairs[0][1]]}\")\n",
    "\n",
    "\n",
    "\n",
    "# nearest_centroid = np.argmin([np.linalg.norm(query - centroid) for centroid in kmeans.cluster_centers_])\n",
    "\n",
    "# # Fine Search (Within the posting list of nearest centroid)\n",
    "# nearest_vectors = [vectors[i] for i in posting_lists[nearest_centroid]]\n",
    "# closest_vector = min(nearest_vectors, key=lambda x: np.linalg.norm(query - x))\n",
    "# print(f\"Actual vector: {query}\")\n",
    "# print(f\"Nearest vector: {closest_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.94669777 0.14702176 0.03122288 0.93221853 0.8269393  0.78845497\n",
      "  0.30489522 0.9944374  0.97145154 0.72643284 0.11840847 0.83041497\n",
      "  0.25410993 0.29975197 0.16046107 0.2680251 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "d:\\Programs\\Anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09518839 0.35055304 0.9398079  ... 0.95536698 0.62127548 0.25900692]\n",
      " [0.60531046 0.0299943  0.18020337 ... 0.84442837 0.30599367 0.31717393]\n",
      " [0.12436448 0.70804765 0.40053623 ... 0.01026024 0.98808191 0.81107594]\n",
      " ...\n",
      " [0.83905272 0.73320902 0.38375223 ... 0.27363777 0.37699509 0.06085107]\n",
      " [0.06235347 0.81673454 0.83788639 ... 0.13500675 0.61006013 0.20705075]\n",
      " [0.5731672  0.76549188 0.87305659 ... 0.68015527 0.97821466 0.63104061]]\n",
      "[[ 55 168  80 ...  63  69 117]\n",
      " [157 240 141 ... 103 160 169]\n",
      " [111 134  58 ...  23  49   5]\n",
      " ...\n",
      " [227  34 191 ... 211 147  18]\n",
      " [ 77  66 245 ... 150 134 118]\n",
      " [ 51 120  81 ... 228 224  70]]\n",
      "[[0.09794463 0.35227004 0.91964504 ... 0.95014297 0.62753523 0.24410977]\n",
      " [0.61093234 0.01673905 0.15575159 ... 0.85667647 0.30498966 0.29733311]\n",
      " [0.12194592 0.71991971 0.39669213 ... 0.01502664 0.98060759 0.81912355]\n",
      " ...\n",
      " [0.82988276 0.71941122 0.39528657 ... 0.29586202 0.37226255 0.06036116]\n",
      " [0.07032296 0.81166198 0.84902497 ... 0.12305148 0.57763475 0.18521893]\n",
      " [0.54778534 0.75887545 0.88783776 ... 0.6724173  0.98195663 0.64319453]]\n",
      "Nearest neighbors: [244 940 274 305 863]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class ProductQuantization:\n",
    "    def __init__(self, m, k):\n",
    "        \"\"\"\n",
    "        Initialize the Product Quantization class.\n",
    "        :param m: Number of subspaces (split vector into m parts)\n",
    "        :param k: Number of clusters per subspace\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.k = k\n",
    "        self.codebooks = []\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        Fit the quantizer to the data.\n",
    "        :param data: Dataset of shape (n_samples, d_features)\n",
    "        \"\"\"\n",
    "        n_samples, d_features = data.shape\n",
    "        assert d_features % self.m == 0, \"Number of features must be divisible by m\"\n",
    "        \n",
    "        self.subvector_dim = d_features // self.m\n",
    "        self.codebooks = []\n",
    "        \n",
    "        for i in range(self.m):\n",
    "            sub_data = data[:, i * self.subvector_dim:(i + 1) * self.subvector_dim]\n",
    "            kmeans = KMeans(n_clusters=self.k, random_state=42).fit(sub_data)\n",
    "            self.codebooks.append(kmeans.cluster_centers_)\n",
    "    \n",
    "    def encode(self, data): #build the index\n",
    "        \"\"\"\n",
    "        Encode the dataset into quantized indices.\n",
    "        :param data: Dataset of shape (n_samples, d_features)\n",
    "        :return: Encoded indices of shape (n_samples, m)\n",
    "        \"\"\"\n",
    "        n_samples, d_features = data.shape\n",
    "        codes = np.zeros((n_samples, self.m), dtype=np.int32) # Stores the nearest centroid for each subspace: if m = 4 -> [[1,5,1,6],[9,1,5,3],...]\n",
    "        \n",
    "        for i in range(self.m):\n",
    "            sub_data = data[:, i * self.subvector_dim:(i + 1) * self.subvector_dim] #partition the data into subspaces\n",
    "            distances = cdist(sub_data, self.codebooks[i]) #\n",
    "            codes[:, i] = np.argmin(distances, axis=1)\n",
    "        \n",
    "        return codes #codes contain the compressed representation of the data\n",
    "    \n",
    "    def decode(self, codes): #used \n",
    "        \"\"\"\n",
    "        Decode the quantized indices back to approximate vectors.\n",
    "        :param codes: Encoded indices of shape (n_samples, m)\n",
    "        :return: Approximate vectors of shape (n_samples, d_features)\n",
    "        \"\"\"\n",
    "        n_samples = codes.shape[0]\n",
    "        decoded_vectors = np.zeros((n_samples, self.m * self.subvector_dim))\n",
    "        \n",
    "        for i in range(self.m):\n",
    "            decoded_vectors[:, i * self.subvector_dim:(i + 1) * self.subvector_dim] = self.codebooks[i][codes[:, i]]\n",
    "        \n",
    "        return decoded_vectors\n",
    "\n",
    "    def search(self, query, codes, top_k=1):\n",
    "        \"\"\"\n",
    "        Perform approximate nearest neighbor search.\n",
    "        :param query: Query vector of shape (1, d_features)\n",
    "        :param codes: Encoded indices of the dataset\n",
    "        :param top_k: Number of nearest neighbors to return\n",
    "        :return: Indices of top_k nearest neighbors\n",
    "        \"\"\"\n",
    "        distances = np.zeros(codes.shape[0])\n",
    "        \n",
    "        for i in range(self.m):\n",
    "            sub_query = query[:, i * self.subvector_dim:(i + 1) * self.subvector_dim]\n",
    "            sub_distances = cdist(sub_query, self.codebooks[i])\n",
    "            distances += sub_distances[0, codes[:, i]]\n",
    "        \n",
    "        return np.argsort(distances)[:top_k]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data\n",
    "    data = np.random.random((1000, 16))  # 1000 samples, 16 dimensions\n",
    "    \n",
    "    query = np.random.random((1, 16))   # 1 query vector\n",
    "    print(query)\n",
    "    pq = ProductQuantization(m=8, k=256)  # 4 subspaces, 256 clusters per subspace\n",
    "    pq.fit(data)\n",
    "    \n",
    "    codes = pq.encode(data)\n",
    "    reconstructed_data = pq.decode(codes)\n",
    "    \n",
    "    # Perform search\n",
    "    neighbors = pq.search(query, codes, top_k=5)\n",
    "    print(data)\n",
    "    print(codes)\n",
    "    print(reconstructed_data)\n",
    "    print(\"Nearest neighbors:\", neighbors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'n_clusters' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvec_db\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VecDB\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMI\n\u001b[1;32m----> 4\u001b[0m db \u001b[38;5;241m=\u001b[39m VecDB(db_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      5\u001b[0m IMI\u001b[38;5;241m.\u001b[39mbuild_index(db)\n",
      "File \u001b[1;32me:\\Github\\vecdb\\vec_db.py:22\u001b[0m, in \u001b[0;36mVecDB.__init__\u001b[1;34m(self, database_file_path, index_file_path, new_db, db_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_path):\n\u001b[0;32m     21\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_path)\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_database(db_size)\n",
      "File \u001b[1;32me:\\Github\\vecdb\\vec_db.py:28\u001b[0m, in \u001b[0;36mVecDB.generate_database\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m     26\u001b[0m vectors \u001b[38;5;241m=\u001b[39m rng\u001b[38;5;241m.\u001b[39mrandom((size, DIMENSION), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_vectors_to_file(vectors)\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_index()\n",
      "File \u001b[1;32me:\\Github\\vecdb\\vec_db.py:128\u001b[0m, in \u001b[0;36mVecDB._build_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    126\u001b[0m no_chunks\u001b[38;5;241m=\u001b[39mmath\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_num_records()\u001b[38;5;241m/\u001b[39mchunk_size)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# Step 1: Coarse Quantization (Clustering)\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m n_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_clusters()               \n\u001b[0;32m    130\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters)\n\u001b[0;32m    131\u001b[0m kmeans\u001b[38;5;241m.\u001b[39mfit(vectors[:\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m])\n",
      "File \u001b[1;32me:\\Github\\vecdb\\vec_db.py:120\u001b[0m, in \u001b[0;36mVecDB._configure_clusters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m num_records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_num_records()\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mmatch\u001b[39;00m num_records:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;241m1_000_000\u001b[39m:\n\u001b[0;32m    113\u001b[0m         n_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;241m20_000_000\u001b[39m:\n\u001b[0;32m    119\u001b[0m         n_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of clusters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_clusters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_clusters\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'n_clusters' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "from vec_db import VecDB\n",
    "from imi import IMI\n",
    "\n",
    "db = VecDB(db_size = 10**3)\n",
    "IMI.build_index(db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMI.retrieve(db,query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

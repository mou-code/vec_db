{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Annotated\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "DB_SEED_NUMBER = 42\n",
    "ELEMENT_SIZE = np.dtype(np.float32).itemsize\n",
    "DIMENSION = 70\n",
    "\n",
    "class VecDB:\n",
    "    def __init__(self, database_file_path = \"saved_db.dat\", index_file_path = \"index.dat\", new_db = True, db_size = None) -> None:\n",
    "        self.db_path = database_file_path\n",
    "        self.index_path = index_file_path\n",
    "        if new_db:\n",
    "            if db_size is None:\n",
    "                raise ValueError(\"You need to provide the size of the database\")\n",
    "            # delete the old DB file if exists\n",
    "            if os.path.exists(self.db_path):\n",
    "                os.remove(self.db_path)\n",
    "            self.generate_database(db_size)\n",
    "    \n",
    "    def generate_database(self, size: int) -> None:\n",
    "        rng = np.random.default_rng(DB_SEED_NUMBER)\n",
    "        vectors = rng.random((size, DIMENSION), dtype=np.float32)\n",
    "        self._write_vectors_to_file(vectors)\n",
    "        self._build_index()\n",
    "    # np.memmap() Parameters:\n",
    "    # First argument: File path to store data\n",
    "    # dtype: Data type (float32 here)\n",
    "    # mode: File access mode\n",
    "    # 'w+': Read/write, create if not exists\n",
    "    # shape: Dimensions of the array\n",
    "    # returns file contents on disk\n",
    "    def _write_vectors_to_file(self, vectors: np.ndarray) -> None:\n",
    "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='w+', shape=vectors.shape)\n",
    "        mmap_vectors[:] = vectors[:]\n",
    "        mmap_vectors.flush()\n",
    "\n",
    "    def _get_num_records(self) -> int:\n",
    "        return os.path.getsize(self.db_path) // (DIMENSION * ELEMENT_SIZE)\n",
    "\n",
    "    def insert_records(self, rows: Annotated[np.ndarray, (int, 70)]):\n",
    "        num_old_records = self._get_num_records()\n",
    "        num_new_records = len(rows)\n",
    "        full_shape = (num_old_records + num_new_records, DIMENSION)\n",
    "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='r+', shape=full_shape)\n",
    "        mmap_vectors[num_old_records:] = rows\n",
    "        mmap_vectors.flush()\n",
    "        #TODO: might change to call insert in the index, if you need\n",
    "        self._build_index()\n",
    "\n",
    "    def get_one_row(self, row_num: int) -> np.ndarray:\n",
    "        # This function is only load one row in memory\n",
    "        try:\n",
    "            offset = row_num * DIMENSION * ELEMENT_SIZE\n",
    "            # [0] is necessary because:\n",
    "            # mmap_vector is 2D array: [[x1, x2, ..., x70]]\n",
    "            # [0] extracts the single row as 1D: [x1, x2, ..., x70]\n",
    "            mmap_vector = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(1, DIMENSION), offset=offset)\n",
    "            return np.array(mmap_vector[0])\n",
    "        except Exception as e:\n",
    "            return f\"An error occurred: {e}\"\n",
    "\n",
    "    def get_all_rows(self) -> np.ndarray:\n",
    "        # Take care this load all the data in memory\n",
    "        num_records = self._get_num_records()\n",
    "        vectors = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(num_records, DIMENSION))\n",
    "        return np.array(vectors)\n",
    "    \n",
    "    def retrieve(self, query: Annotated[np.ndarray, (1, DIMENSION)], top_k = 5):\n",
    "        scores = []\n",
    "        num_records = self._get_num_records()\n",
    "        # here we assume that the row number is the ID of each vector\n",
    "        for row_num in range(num_records):\n",
    "            vector = self.get_one_row(row_num)\n",
    "            score = self._cal_score(query, vector)\n",
    "            scores.append((score, row_num))\n",
    "        # here we assume that if two rows have the same score, return the lowest ID\n",
    "        scores = sorted(scores, reverse=True)[:top_k]\n",
    "        return [s[1] for s in scores]\n",
    "    \n",
    "    def _cal_score(self, vec1, vec2):\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm_vec1 = np.linalg.norm(vec1)\n",
    "        norm_vec2 = np.linalg.norm(vec2)\n",
    "        cosine_similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "        return cosine_similarity\n",
    "\n",
    "def _build_index(self, num_clusters=10, num_subspaces=7, subspace_dim=10):\n",
    "        \n",
    "        # Placeholder for index building logic\n",
    "        ### IVF\n",
    "        # Apply k means clustering to all vectors -> return the cluster centroids \n",
    "        # store the clusters and their centroids in a array or dictionary?\n",
    "        # Within each cluster:\n",
    "        # loop over each vector \n",
    "        # create an array of subspace arrays (parameter) subspaces[[   [subvector1 of array 1],[subvector1 of array 2]   ] , [],...]\n",
    "        # apply k-means clustering for each subspace[0] subspace[1] etc..\n",
    "        # Generate the codebook\n",
    "        \n",
    "        \n",
    "        # Build the PQ-IVF index.\n",
    "        # Args:\n",
    "        #     num_clusters (int): Number of clusters for the inverted file (IVF).\n",
    "        #     num_subspaces (int): Number of subspaces for product quantization.\n",
    "        #     subspace_dim (int): Dimension of each subspace.\n",
    "        \n",
    "        if DIMENSION % num_subspaces != 0:\n",
    "            raise ValueError(\"DIMENSION must be divisible by num_subspaces.\")\n",
    "\n",
    "        # 1. Perform k-means clustering on full vectors\n",
    "        all_vectors = self.get_all_rows()\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=DB_SEED_NUMBER)\n",
    "        cluster_assignments = kmeans.fit_predict(all_vectors)\n",
    "        self.cluster_centroids = kmeans.cluster_centers_\n",
    "\n",
    "        # 2. Build the inverted file\n",
    "        self.inverted_file = {i: [] for i in range(num_clusters)}\n",
    "        for idx, cluster_id in enumerate(cluster_assignments):\n",
    "            self.inverted_file[cluster_id].append(idx)\n",
    "\n",
    "        # 3. Apply Product Quantization\n",
    "        self.pq_codebooks = []\n",
    "        subspaces = np.split(all_vectors, num_subspaces, axis=1)  # Split vectors into subspaces\n",
    "        for subspace in subspaces:\n",
    "            sub_kmeans = KMeans(n_clusters=256, random_state=DB_SEED_NUMBER)  # 256 centroids per subspace\n",
    "            sub_kmeans.fit(subspace)\n",
    "            self.pq_codebooks.append(sub_kmeans.cluster_centers_)\n",
    "        \n",
    "        # 4. Encode all vectors\n",
    "        self.pq_codes = self._encode_vectors(all_vectors, cluster_assignments)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VecDB' object has no attribute '_build_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m db \u001b[38;5;241m=\u001b[39m VecDB(db_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      3\u001b[0m all_db \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39mget_all_rows()\n\u001b[0;32m      5\u001b[0m res \u001b[38;5;241m=\u001b[39m run_queries(db, all_db, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m, in \u001b[0;36mVecDB.__init__\u001b[1;34m(self, database_file_path, index_file_path, new_db, db_size)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_path):\n\u001b[0;32m     19\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_path)\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_database(db_size)\n",
      "Cell \u001b[1;32mIn[6], line 26\u001b[0m, in \u001b[0;36mVecDB.generate_database\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m     24\u001b[0m vectors \u001b[38;5;241m=\u001b[39m rng\u001b[38;5;241m.\u001b[39mrandom((size, DIMENSION), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_vectors_to_file(vectors)\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_index()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'VecDB' object has no attribute '_build_index'"
     ]
    }
   ],
   "source": [
    "db = VecDB(db_size = 10**2)\n",
    "\n",
    "all_db = db.get_all_rows()\n",
    "\n",
    "res = run_queries(db, all_db, 5, 10)\n",
    "print(eval(res))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
